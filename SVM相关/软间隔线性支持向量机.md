<div align=center>
<font size="6"><b>《SVM笔记系列之五》软间隔线性支持向量机</b></font> 
</div>

# 前言
**在以前的文章中，我们介绍了支持向量机的基本表达式，那是基于硬间隔线性支持向量机的，即是假设数据是完全线性可分的，在数据是近似线性可分的时候，我们不能继续使用硬间隔SVM了，而是需要采用软间隔SVM，在这里我们简单介绍下软间隔线性支持向量机。本人无专业的数学学习背景，只能在直观的角度上解释这个问题，如果有数学专业的朋友，还望不吝赐教。**
**如有误，请联系指正。转载请注明出处。**
*联系方式：*
**e-mail**: [`FesianXu@163.com`](FesianXu@163.com)
**QQ**: `973926198`
**github**: [`https://github.com/FesianXu`](https://github.com/FesianXu)
**有关代码开源**: [click][click]

*****************************************************************************

# 软间隔最大化
在文章[《SVM的拉格朗日函数表示以及其对偶问题》][ref_1]和[《SVM支持向量机的目的和起源》][ref_2]中，我们推导了SVM的基本公式，那时的基本假设之一就是**数据是完全线性可分的**，即是总是存在一个超平面$W^TX+b$可以将数据完美的分开，但是正如我们在[《SVM的拉格朗日函数表示以及其对偶问题》][ref_1]中最后结尾所说的：
> 但是，在现实生活中的数据往往是或本身就是非线性可分但是近似线性可分的，或是线性可分但是具有噪声的，以上两种情况都会导致在现实应用中，硬间隔线性支持向量机变得不再实用

因此，我们引入了**软间隔线性支持向量机**这个概念，**硬间隔**和**软间隔**的区别如下图所示：
![hard_margin][hard_margin]
![soft_margin][soft_margin]

我们的解决方案很简单，就是在软间隔SVM中，我们的分类超平面既要**能够尽可能地将数据类别分对，又要使得支持向量到超平面的间隔尽可能地小**。具体来说，因为线性不可分意味着某些样本点不能满足函数间隔大于等于1的条件，即是$\exists i, 1-y_i(W^Tx_i+b) > 0$。解决方案就是通过对每一个样本点$(x_i, y_i)$引入一个松弛变量$\xi_i \geq 0$，对于那些不满足约束条件的样本点，使得函数间隔加上松弛变量之后大于等于1，于是我们的约束条件就变为了：
$$
y_i(W^T x_i+b)+\xi_i \geq 1 \\
= y_i(W^T x_i+b) \geq 1-\xi_i
\tag{1.1}
$$
图像表示如：
![soft_margin_xi][soft_margin_xi]
超平面两侧对称的虚线为支持向量，支持向量到超平面的间隔为1。**在硬间隔SVM中本应该是在虚线内侧没有任何的样本点的，而在软间隔SVM中，因为不是完全的线性可分，所以虚线内侧存在有样本点，通过向每一个在虚线内侧的样本点添加松弛变量$\xi_i$，将这些样本点搬移到支持向量虚线上。而本身就是在虚线外的样本点的松弛变量则可以设为0。**
于是，给每一个松弛变量赋予一个代价$\xi_i$，我们的目标函数就变成了：
$$
f(W, \xi) = \frac{1}{2}||W||^2+C \sum_{i=1}^N\xi_i \\
i = 1,2, \cdots,N
\tag{1.2}
$$
其中$C > 0$称为**惩罚参数**，C值大的时候对误分类的惩罚增大，C值小的时候对误分类的惩罚减小，$(1.2)$有两层含义：使得$\frac{1}{2}||W||^2$尽量小即是间隔尽可能大，同时使得误分类的数量尽量小，C是调和两者的系数。
于是我们的软间隔SVM的问题可以描述为：
$$
\min_{W,b,\xi} \frac{1}{2}||W||^2+C \sum_{i=1}^N\xi_i \\
s.t.　　y_i(W^T x_i+b) \geq 1-\xi_i \\
\xi_i \geq 0 \\
i = 1,2, \cdots, N
\tag{1.3}
$$
表述为标准形式：
$$
\min_{W,b,\xi} \frac{1}{2}||W||^2+C \sum_{i=1}^N\xi_i \\
s.t.　　1-\xi_i-y_i(W^T x_i+b) \leq 0 \\
-\xi_i \leq 0 \\
i = 1,2, \cdots, N
\tag{1.4}
$$

*****************************************************************************

# 软间隔SVM的拉格朗日函数表述和对偶问题
我们采用[《SVM的拉格朗日函数表示以及其对偶问题》][ref_1]中介绍过的相似的方法，将$(1.4)$得到其对偶问题。过程如下：
将$(1.4)$转换为其拉格朗日函数形式：
$$
L(W, b, \xi, \alpha, \beta) = 
\frac{1}{2}||W||^2+C \sum_{i=1}^N\xi_i+\sum_{i=1}^N \alpha_i(1-\xi_i-y_i(W^T x_i+b))- \sum_{i=1}^N \beta_i \xi_i \\
= \frac{1}{2}||W||^2+C \sum_{i=1}^N\xi_i+ \sum_{i=1}^N \alpha_i - \sum_{i=1}^N \alpha_i \xi_i - \sum_{i=1}^N \alpha_i y_i(W^T x_i+b) - \sum_{i=1}^N \beta_i \xi_i \\
\tag{2.1}
$$
原问题可以表述为（具体移步[《最优化问题的对偶问题》][ref_3]）:
$$
\min_{W,b,\xi} \max_{\alpha, \beta} L(W, b, \xi, \alpha, \beta)
\tag{2.2}
$$
得到其对偶问题为：
$$
\max_{\alpha, \beta} \min_{W, b, \xi} L(W, b, \xi, \alpha, \beta)
\tag{2.3}
$$
我们先求对偶问题$\theta_D(\alpha, \beta) = \min_{W, b, \xi} L(W, b, \xi, \alpha, \beta)$，根据KKT条件(具体移步[《拉格朗日乘数法和KKT条件的直观解释》][ref_4])，我们有：
$$
\nabla_{W} L(W, b, \xi, \alpha, \beta) = W-\sum_{i=1}^N\alpha_i y_i x_i = 0
\tag{2.4}
$$
$$
\nabla_{b} L(W, b, \xi, \alpha, \beta) = \sum_{i=1}^N \alpha_i y_i = 0
\tag{2.5}
$$
$$
\nabla_{\xi_i} L(W, b, \xi, \alpha, \beta) = C-\alpha_i-\beta_i = 0
\tag{2.6}
$$
$$
\alpha_i \geq 0, \beta_i \geq 0
\tag{2.7}
$$
整理得到：
$$
W = \sum_{i=1}^N\alpha_i y_i x_i \\
\sum_{i=1}^N \alpha_i y_i = 0 \\
C = \alpha_i+\beta_i
\tag{2.8}
$$
将$(2.8)$代入$(2.1)$，有：
$$
L(W, b, \xi, \alpha, \beta) = \frac{1}{2} \sum_{i=1}^N\alpha_i y_i x_i \sum_{j=1}^N\alpha_j y_j x_j+(\alpha_i+\beta_i)\sum_{i=1}^N \xi_i+\sum_{i=1}^N \alpha_i - \sum_{i=1}^N \alpha_i \xi_i - \sum_{i=1}^N \beta_i \xi_i - \sum_{i=1}^N \alpha_iy_i(\sum_{j=1}^N\alpha_j y_j x_j \cdot x_i +b) \\
= -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)+\sum_{i=1}^N \alpha_i
\tag{2.9}
$$
所以问题变为：
$$
\max_{\alpha, \beta} \theta_D(\alpha, \beta) = \max_{\alpha} -\frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)+\sum_{i=1}^N \alpha_i
\tag{2.10}
$$
表述为最小化问题：
$$
\min_{\alpha} \frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)-\sum_{i=1}^N \alpha_i \\
s.t.　　　\sum_{i=1}^N \alpha_i y_i = 0 \\
\alpha_i \geq 0 \\
\beta_i \geq 0 \\
C = \alpha_i+\beta_i
\tag{2.11}
$$
通过将$\beta_i = C-\alpha_i$，$(2.11)$可以化为：
$$
\min_{\alpha} \frac{1}{2} \sum_{i=1}^N\sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)-\sum_{i=1}^N \alpha_i \\
s.t.　　　\sum_{i=1}^N \alpha_i y_i = 0 \\
0 \leq \alpha_i \leq C
\tag{2.12}
$$
对比文章[《SVM的拉格朗日函数表示以及其对偶问题》][ref_1]中的硬间隔SVM的最终的表达式：
$$
\min_{\alpha}
\frac{1}{2}\sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_jy_iy_j(x_i \cdot x_j)- \sum_{i=1}^N\alpha_i
$$
$$
s.t. \ \sum_{i=1}^N\alpha_iy_i=0
$$
$$
\alpha_i \geq0,i=1,\cdots,N
\tag{2.13}
$$
不难发现软间隔SVM只是在对拉格朗日乘子$\alpha_i$的约束上加上了一个上界$C$。
我们以后都会利用$(2.12)$求解，接下来我们在**SMO算法**中，也将对式子$(2.12)$进行求解。

*****************************************************************************

# 引用
1. [《SVM的拉格朗日函数表示以及其对偶问题》 CSDN][ref_1]
2. [《SVM支持向量机的目的和起源》 CSDN][ref_2]
3. [《最优化问题的对偶问题》 CSDN][ref_3]
4. [《拉格朗日乘数法和KKT条件的直观解释》 CSDN][ref_4]
5. [《统计学习方法》 豆瓣][ref_5]


[ref_1]: http://blog.csdn.net/LoseInVain/article/details/78636285
[ref_2]: http://blog.csdn.net/LoseInVain/article/details/78636176
[ref_3]: http://blog.csdn.net/loseinvain/article/details/78636341
[ref_4]: http://blog.csdn.net/loseinvain/article/details/78624888
[ref_5]: https://book.douban.com/subject/10590856/

[soft_margin]: ./imgs/soft_margin_svm.png
[hard_margin]: ./imgs/hard_margin_svm.png
[soft_margin_xi]: ./imgs/soft_margin_xi.png

[click]: https://github.com/FesianXu/AI_Blog/tree/master/SVM相关

