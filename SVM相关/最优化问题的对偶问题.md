<div align=center>
<font size="6"><b>《SVM笔记系列之四》最优化问题的对偶问题</b></font> 
</div>

# 前言
**在SVM的推导中，在得到了原问题的拉格朗日函数表达之后，是一个最小最大问题，通常会将其转化为原问题的对偶问题即是最大最小问题进行求解，我们这里简单介绍下最优化问题的对偶问题。本人无专业的数学学习背景，只能在直观的角度上解释这个问题，如果有数学专业的朋友，还望不吝赐教。注意，本文应用多限于SVM，因此会比较狭隘。**
**如有谬误，请联系指正。转载请注明出处。**

*联系方式：*
**e-mail**: [`FesianXu@163.com`](FesianXu@163.com)
**QQ**: `973926198`
**github**: [`https://github.com/FesianXu`](https://github.com/FesianXu)
**有关代码开源**: [click][click]

*****************************************************************************

# 最优化问题
最优化问题研究的是当函数（目标函数）在给定了一系列的约束条件下的最大值或最小值的问题，一般来说，一个最优化问题具有以下形式：
$$
\min_{x \in R^n} f(x) \\
s.t. 　　g_i(x) \leq 0 ,i=1,2,\cdots, N\\
　　　   h_j(x) = 0, j=1,2,\cdots, M
\tag{1.1}
$$

最优化问题可以根据**目标函数和约束条件的类型**进行分类: 
1. 如果目标函数和约束条件都为变量的线性函数, 称该最优化问题为**线性规划**;
2. 如果目标函数为变量的二次函数, 约束条件为变量的仿射函数, 称该最优化问题为**二次规划**; 
3. 如果目标函数或者约束条件为变量的非线性函数, 称该最优化问题为**非线性规划**.

*****************************************************************************

# 对偶问题
最优化问题存在对偶问题，所谓对偶问题，源于这个思想：
> 原始问题比较难以求解，通过构建其对偶问题，期望解决这个对偶问题得到其原问题的下界（在**弱对偶**情况下，对于最小化问题来说），或者得到原问题的解（**强对偶**情况下）。

在SVM中，因为其属于**凸优化问题**，因此是强对偶问题，可以通过构建对偶问题解决得到原问题的解。
我们举一个线性规划中一个经典问题，描述如下：
> 某工厂有两种原料A、B，而且能用其生产两种产品：
  1. 生产第一种产品需要2个A和4个B，能够获利6；
  2. 生产第二种产品需要3个A和2个B，能够获利4；
  此时共有100个A和120个B，问该工厂最多获利多少？

可以简单得到其问题的数学表达式为：
$$
\max_{x_1, x_2} 6x_1+4x_2 \\
s.t.　　2x_1+3x_2 \leq 100 \\
　　　4x_1+2x_2 \leq 120
\tag{2.1}
$$
当然，得到这个式子的根据就是最大化其卖出去的产品的利润。但是，如果只问收益的话，明显地，还可以考虑卖出原材料A和B的手段，前提就是卖出原材料的盈利会比生产商品盈利高，假设产品A和产品B的单价为$w_1$和$w_2$，从这个角度看，只要最小化购买原材料的价格，我们就可以得出另一个数学表达式：
$$
\min_{w_1, w_2} {100w_1+120w_2} \\
s.t.　　2w_1+4w_2  \geq 6 \\
　　　  3w_1+2w_2 \geq 4
\tag{2.2}
$$
**其实，我们可以发现这其实是极大极小问题和其对偶问题，极小极大问题**。

*****************************************************************************

# 一些定义

## 原始问题
我们要讨论原问题和对偶问题，就需要一些定义，我们给出原始问题的非拉格朗日函数表达形式如式子$(1.1)$所示，引进其广义拉格朗日函数（详见文章[《拉格朗日乘数法和KKT条件的直观解释》][ref_4]）：
$$
L(x, \alpha, \beta)_{x \in R^n} = f(x)+\sum_{i=1}^N \alpha_i g_i(x)+\sum_{j=1}^M \beta_j h_j(x)
\tag{3.1}
$$
其中$x=(x^{(1)}, x^{(2)}, \cdots, x^{(n)})^T \in R^n$，而$\alpha_i$和$\beta_j$是拉格朗日乘子，其中由**KKT条件**有$\alpha_i \geq 0$，考虑关于x的函数：
$$
\theta_P(x) = \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta)
\tag{3.2}
$$
这里下标$P$用以表示这个是原始问题。
联想到我们在文章[《SVM的拉格朗日函数表示以及其对偶问题》][ref_5]中一些关于对偶问题的讨论，我们知道其实$(3.2)$中的$\theta_P(x)$其实就表示了$(1.1)$中的原问题的目标函数和其约束条件，这里再探讨一下：
**假设我们存在一个x，使得x违反原始问题的约束条件，从而有$g_i(x) > 0$或者$h_j(x) \neq 0$，那么我们可以推论出：**
$$
\theta_P(x) = \max_{\alpha, \beta; \alpha_i \geq 0} [f(x)+\sum_{i=1}^N \alpha_i g_i(x)+ \sum_{j=1}^M \beta_j h_j(x)] = + \infty
\tag{3.3}
$$
为什么呢？**因为若存在某个i使得$g_i(x) > 0$， 那么就可以令$\alpha_i \rightarrow +\infty$使得$\theta_P(x$)取得无穷大这个“最大值”；同样的，若存在一个j使得$h_j(x) \neq 0$， 那么就总是可以使得$\beta_j$让$\beta_j h_j(x) \rightarrow +\infty$， 而其他各个$\alpha_i$和$\beta_j$均取为0（满足约束条件的拉格朗日乘子取为0）。这样，只有对于满足约束条件的i和j，才会有$\theta_P(x)=f(x)$成立。**于是我们有这个分段表达式：
$$
\theta_P(x) =
\begin{cases}
f(x) & x满足原始问题约束 \\
+ \infty & 其他
\end{cases}
\tag{3.4}
$$
所以，如果是最小化问题，我们有极小极大问题$(3.5)$:
$$
\min_{x} \theta_P(x) = \min_{x} \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta)
\tag{3.5}
$$
**其与式子$(1.1)$是完全等价的，有着同样的解**。这样一来，我们就把原始的最优化问题转换为了广义拉格朗日函数的极小极大问题，为了后续讨论方便，我们记：
$$
p^* = \min_{x} \theta_P(x)
\tag{3.6}
$$
其中$p^*$为问题的解。

## 极小极大问题的对偶， 极大极小问题
我们定义：
$$
\theta_{D} (\alpha, \beta) = \min_{x} L(x, \alpha, \beta)
\tag{3.7}
$$
在考虑极大化$(3.7)$有：
$$
\max_{\alpha, \beta; \alpha_i \geq 0} \theta_D(\alpha, \beta)=\max_{\alpha, \beta; \alpha_i \geq 0} \min_{x} L(x, \alpha, \beta)
\tag{3.8}
$$
式子$(3.8)$称为广义拉格朗日函数的极大极小问题，将其变成约束形式，为：
$$
\max_{\alpha, \beta} \theta_D(\alpha, \beta)=\max_{\alpha, \beta} \min_{x} L(x, \alpha, \beta) \\
s.t. 　　\alpha_i \geq 0
\tag{3.9}
$$
式子$(3.9)$被称为原问题的对偶问题，定义其最优解为：
$$
d^* = \max_{\alpha, \beta} \theta_D(\alpha, \beta)
\tag{3.10}
$$
实际上，通过这种方法我们可以将式子$(2.1)$转化为式子$(2.2)$，也就是将原问题转化为对偶问题，有兴趣的朋友可以自行尝试。

*****************************************************************************

# 原始问题和对偶问题的关系
正如前面所谈到的，原始问题的解和对偶问题的解存在一定的关系，对于任意的$\alpha, x, \beta$，我们有：
$$
\theta_D(\alpha, \beta)=\min_{x} L(x, \alpha, \beta) \leq L(x, \alpha, \beta) \leq 
\max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta)=\theta_P(x)
\tag{4.1}
$$
等价于：
$$
\theta_D(\alpha, \beta) \leq \theta_P(x)
\tag{4.2}
$$
注意，式子$(4.2)$对于所有的$x, \alpha, \beta$都成立，因为原始问题和对偶问题均有最优解，所以有：
$$
\max_{\alpha, \beta; \alpha_i \geq 0} \theta_D(\alpha, \beta) \leq \min_{x} \theta_P(x)
\tag{4.3}
$$
容易得到：
$$
d^* = \max_{\alpha, \beta; \alpha_i \geq 0} \min_{x} L(x, \alpha, \beta) \leq \min_{x} \max_{\alpha, \beta; \alpha_i \geq 0} L(x, \alpha, \beta) = p^*
\tag{4.4}
$$
由此我们得到了在最小化问题中$d^* \leq p^*$的结论，这个称为**弱对偶**。**弱对偶指出，解决最小化问题的对偶问题可以得到原问题的解的下界**。

既然有弱对偶就会存在**强对偶**。强对偶指的是$d^*=p^*$的情况，在某些情况下，原始问题和对偶问题的解相同，这时可以用解决对偶问题来代替原始问题，下面以定理的方式给出强对偶成立的重要条件而不予以证明：
> 考虑原始问题$(1.1)$和对偶问题$(3.9)$，假设$f(x)$和$g_i(x)$都是凸函数， $h_j(x)$是仿射函数，并且不等式约束$g_i(x)$是严格可行的，既存在$x$，对所有$i$有$g_i(x) < 0 $，则存在$x^*,\alpha^*,\beta^*$，使得$x^*$是原始问题的解，$\alpha^*,\beta^*$是对偶问题的解（满足这个条件的充分必要条件就是$x^*, \alpha^*, \beta^*$满足KKT条件[^1]），并且：
> $p^* = d^* = L(x^*, \alpha^*, \beta^*)$



*****************************************************************************

# 引用
1. [最优化问题学习笔记1-对偶理论 CSDN][ref_1]
2. [《统计学习方法》 豆瓣][ref_2]
3. [如何理解对偶问题？ feng liu的回答][ref_3]
4. [《拉格朗日乘数法和KKT条件的直观解释》 CSDN][ref_4]
5. [SVM的拉格朗日函数表示以及其对偶问题 CSDN][ref_5]

[^1]: 见[《拉格朗日乘数法和KKT条件的直观解释》](http://blog.csdn.net/loseinvain/article/details/78624888)

[ref_1]: http://blog.csdn.net/qq_34531825/article/details/52872819
[ref_2]: https://book.douban.com/subject/10590856/
[ref_3]: https://www.zhihu.com/question/27057384
[ref_4]: http://blog.csdn.net/loseinvain/article/details/78624888
[ref_5]: http://blog.csdn.net/loseinvain/article/details/78636285

[click]: https://github.com/FesianXu/AI_Blog/tree/master/SVM相关

