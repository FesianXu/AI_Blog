<div align=center>
<font size="6"><b>《SVM笔记系列之一》SVM的目的和起源</b></font> 
</div>

# 前言
**支持向量机是常用的，泛化性能佳的，而且可以应用核技巧的机器学习算法，在深度学习流行前是最被广泛使用的机器学习算法之一，就算是深度学习流行的现在，支持向量机也由于其高性能，较低的计算复杂度而被人们广泛应用。这里结合李航博士的《统计学习方法》一书的推导和林轩田老师在《机器学习技法》中的讲解，谈谈自己的认识。**
**如有谬误，请联系指正。转载请注明出处。**
*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`



# SVM的起源
　　**支持向量机(Support Vector Machine, SVM)**是一种被广泛使用的机器学习算法，自从被Vapnik等人提出来之后便被广泛使用和发展。传统的支持向量机一般是**二类分类器**，其基本出发点很简单，就是**找到一个策略，能够让线性分类器的分类超平面能够最大程度的把两类的样本最好地分割开**，这里我们讨论下什么叫做**最好地分割开**，和**实现这个对整个分类器的意义。**

## 最好地分割数据
　　在进行接下来的讨论之前，为了简化我们的讨论从而直面问题所在，我们进行以下假设：
> 1） 我们现在的两类数据是线性可分的， 也就是总是存在一个超平面$W^TX+b$可以将数据完美的分开。
> 2） 我们的数据维度是二维的，也就是特征维只有两个，类标签用+1， -1表示，这样方便我们绘制图像。

　　我在前篇博文《感知器》中已经介绍到了感知器这一简单的线性分类器。感知器很原始，只能对线性可分的数据进行准确分割，而且由于其激活函数选用的是阶跃函数，因此不能通过梯度的方法进行参数更新，而是只能采用**错误驱动的策略**进行参数更新，这样我们的超平面其实是**不确定的**，因为其取决于具体随机到的是哪个样本点进行更新，这是一个不稳定的结果。而且，由于采用了这种参数更新策略，感知器的超平面即使是能够将线性数据完美地分割开，也会出现超平面非常接近某一个类的样本，而偏离另一个类的样本的这种情况