<div align=center>
<font size="6"><b>《SVM笔记系列之一》SVM的目的和起源</b></font> 
</div>

# 前言
**支持向量机是常用的，泛化性能佳的，而且可以应用核技巧的机器学习算法，在深度学习流行前是最被广泛使用的机器学习算法之一，就算是深度学习流行的现在，支持向量机也由于其高性能，较低的计算复杂度而被人们广泛应用。这里结合李航博士的《统计学习方法》一书的推导和林轩田老师在《机器学习技法》中的讲解，谈谈自己的认识。**
**如有谬误，请联系指正。转载请注明出处。**
*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`
**有关代码开源**: [click][click]



# SVM的起源
　　**支持向量机(Support Vector Machine, SVM)**是一种被广泛使用的机器学习算法，自从被Vapnik等人提出来之后便被广泛使用和发展。传统的支持向量机一般是**二类分类器**，其基本出发点很简单，就是**找到一个策略，能够让线性分类器的分类超平面能够最大程度的把两类的样本最好地分割开**，这里我们讨论下什么叫做**最好地分割开**，和**实现这个对整个分类器的意义。**

## 最好地分割数据
　　在进行接下来的讨论之前，为了简化我们的讨论从而直面问题所在，我们进行以下假设：
> 1） 我们现在的两类数据是线性可分的， 也就是总是存在一个超平面$W^TX+b$可以将数据完美的分开。
> 2） 我们的数据维度是二维的，也就是特征维只有两个，类标签用+1， -1表示，这样方便我们绘制图像。

　　我在前篇博文[《机器学习系列之 感知器模型》][perceptron]中已经介绍到了感知器这一简单的线性分类器。感知器很原始，只能对线性可分的数据进行准确分割，而且由于其激活函数选用的是阶跃函数，因此不能通过梯度的方法进行参数更新，而是只能采用**错误驱动的策略**进行参数更新，这样我们的超平面其实是**不确定的**，因为其取决于具体随机到的是哪个样本点进行更新，这是一个不稳定的结果。而且，由于采用了这种参数更新策略，感知器的超平面即使是能够将线性数据完美地分割开，也经常会出现超平面非常接近某一个类的样本，而偏离另一个类的样本的这种情况，特别是在真实情况下的数据是叠加了噪声的情况下。
　　如下图所示，其中绿线是感知器的运行结果，因为其算法的不稳定性，所以每次的结果都可能不同，选中的这一次我们可以看出来虽然绿线**将两类数据完美地分割开了，但是和蓝色样本很接近，如果新来的测试样本叠加一个噪声，这个超平面就很容易将它分类错误，而最佳分类面粉色线则对噪声有着更好地容忍。**
![best_divide][best_divide]

## 样本噪声
　　刚才我们谈到了样本集上叠加的**噪声**，噪声广泛存在于真实数据集中，无法避免，因此我们的分类超平面要能够对噪声进行一定的容忍。一般我们假设噪声为高斯噪声，如下图所示：
![noise][noise]
　　其中红点为实际的采样到的样本位置$P_{sample}$，而蓝点是可能的样本的实际位置$P_{actual}$，因为噪声$N$的叠加才使得其偏离到了红点位置，其中蓝点的位置满足高斯分布。
$$
P_{sample} = P_{actual}+N, N \sim N(\mu, \sigma^2)
$$

## 最佳分类超平面
　　也就是说我们根据$P_{sample}$点训练出来的感知器的分类器超平面很可能会出现**可以完美地划分$P_{sample}$点，但是却不能正确地划分对新来的测试样本的现象。因为新来的样本很可能位于蓝色的样本点的位置，也就是表现出了严重的过拟合现象**， 而我们的**支持向量机**的机制可以很好地减免这种现象，具有更好的泛化能力。我们用几张图来表述下导致这种过拟合的原因：
![overfit][overfit]
Figure 1, 感知器分类超平面能将线性可分的样本完美分割，但是由于样本叠加了高斯噪声$N$，所以当测试样本的数据出现在超平面“穿过”的“绿圈”之内时，就可能会出现错分的情况，这就是**过拟合**的一种表现。
![svm_divide][svm_divide]
Figure 2,假设我们的样本集都是独立同分布采样的，那么其叠加的高斯噪声$N$应该都是相同分布$N \sim N(\mu, \sigma^2)$的，因此这个绿圈的大小应该都是相同的，因此最佳的分类超平面应该是可以和距离它最接近的若干个样本的边界相切的。我们把最接近超平面的若干个样本点称为**支持向量**，支持向量和超平面的距离越远，相当于我们可以容忍的噪声的高斯分布的方差越小，泛化性能越好。**注意，这里的高斯分布的方差是我们假设的，不一定是实际数据集叠加的高斯噪声分布的方差，但是假设的越大，总是能带来更好的泛化能力。**

# SVM提出
　　我们在上面谈到了最佳分类超平面应能够使得支持向量距离超平面的距离最大，这个就是**支持向量机**的基本机制的最优化的目标，我们需要解决这个问题就必须要先数学形式化我们这个目的，只有这样才能进行最优化和求解。
  
## 数学形式化表述

## 函数间隔和几何间隔



[click]: https://github.com/FesianXu/AI_Blog/tree/master/SVM%E7%9B%B8%E5%85%B3
[perceptron]: http://blog.csdn.net/LoseInVain/article/details/78430585
[best_divide]: ./imgs/best_divide.png
[noise]: ./imgs/noise.png
[overfit]: ./imgs/overfit.png
[svm_divide]: ./imgs/svm_divide.png


