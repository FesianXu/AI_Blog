<div align=center>
<font size="6"><b>《SVM笔记系列之一》SVM的目的和起源</b></font> 
</div>

# 前言
**支持向量机是常用的，泛化性能佳的，而且可以应用核技巧的机器学习算法，在深度学习流行前是最被广泛使用的机器学习算法之一，就算是深度学习流行的现在，支持向量机也由于其高性能，较低的计算复杂度而被人们广泛应用。这里结合李航博士的《统计学习方法》一书的推导和林轩田老师在《机器学习技法》中的讲解，谈谈自己的认识。**
**如有谬误，请联系指正。转载请注明出处。**
*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`
**有关代码开源**: [click][click]



# SVM的起源
　　**支持向量机(Support Vector Machine, SVM)**是一种被广泛使用的机器学习算法，自从被Vapnik等人提出来之后便被广泛使用和发展。传统的支持向量机一般是**二类分类器**，其基本出发点很简单，就是**找到一个策略，能够让线性分类器的分类超平面能够最大程度的把两类的样本最好地分割开**，这里我们讨论下什么叫做**最好地分割开**，和**实现这个对整个分类器的意义。**

## 最好地分割数据
　　在进行接下来的讨论之前，为了简化我们的讨论从而直面问题所在，我们进行以下假设：
> 1） 我们现在的两类数据是线性可分的， 也就是总是存在一个超平面$W^TX+b$可以将数据完美的分开。
> 2） 我们的数据维度是二维的，也就是特征维只有两个，类标签用+1， -1表示，这样方便我们绘制图像。

　　我在前篇博文[《机器学习系列之 感知器模型》][perceptron]中已经介绍到了感知器这一简单的线性分类器。感知器很原始，只能对线性可分的数据进行准确分割，而且由于其激活函数选用的是阶跃函数，因此不能通过梯度的方法进行参数更新，而是只能采用**错误驱动的策略**进行参数更新，这样我们的超平面其实是**不确定的**，因为其取决于具体随机到的是哪个样本点进行更新，这是一个不稳定的结果。而且，由于采用了这种参数更新策略，感知器的超平面即使是能够将线性数据完美地分割开，也经常会出现超平面非常接近某一个类的样本，而偏离另一个类的样本的这种情况，特别是在真实情况下的数据是叠加了噪声的情况下。
　　如下图所示，其中绿线是感知器的运行结果，因为其算法的不稳定性，所以每次的结果都可能不同，选中的这一次我们可以看出来虽然绿线**将两类数据完美地分割开了，但是和蓝色样本很接近，如果新来的测试样本叠加一个噪声，这个超平面就很容易将它分类错误，而最佳分类面粉色线则对噪声有着更好地容忍。**
![best_divide][best_divide]

## 样本噪声
　　刚才我们谈到了样本集上叠加的**噪声**，噪声广泛存在于真实数据集中，无法避免，因此我们的分类超平面要能够对噪声进行一定的容忍。一般我们假设噪声为高斯噪声，如下图所示：
![noise][noise]
　　其中红点为实际的采样到的样本位置$P_{sample}$，而蓝点是可能的样本的实际位置$P_{actual}$，因为噪声$N$的叠加才使得其偏离到了红点位置，其中蓝点的位置满足高斯分布。





[click]: https://github.com/FesianXu/AI_Blog/tree/master/SVM%E7%9B%B8%E5%85%B3
[perceptron]: http://blog.csdn.net/LoseInVain/article/details/78430585
[best_divide]: ./imgs/best_divide.png
[noise]: ./imgs/noise.png
