<div align=center>
<font size="6"><b>《SVM笔记系列之三》最优化问题和（广义）拉格朗日乘数法</b></font> 
</div>

# 前言
**在SVM的推导中，出现了核心的一个最优化问题，这里我们简单介绍下最优化问题，特别是带有约束的最优化问题，并且引入拉格朗日乘数法和广义拉格朗日乘数法，用于解决带约束的最优化问题。**
**如有谬误，请联系指正。转载请注明出处。**
*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`
**有关代码开源**: [click][click]

*****************************************************************************


# 最优化问题
我们在高中，包括在高数中都会经常遇到求解一个函数的最小值，最大值之类的问题，这类问题就是属于最优化问题。比如给出下列一个不带有约束的最优化问题：
$$
\min_{x} 3x^2+4x+5, x \in R
\tag{1.1}
$$
其中的$3x^2+4x+5$我们称为**目标函数(objective  function)**。这样的问题，直接利用**罗尔定理（Rolle's theorem）**求出其鞍点，又因为其为凸函数而且可行域是整个$R$，求出的鞍点便是最值点，这个是对于无约束最优化问题的解题套路。
如果问题带有约束条件，那么就变得不一样了，如：
$$
\min_{x, y} 3xy^2
$$
$$
s.t.　　　4x+5y = 10
\tag{1.2}
$$
因为此时的约束条件是**仿射函数（affine function）**[^1]，所以可以利用换元法将$x$表示为$y$的函数，从而将目标函数变为无约束的形式，然后利用罗尔定理便可以求出最值点了。
然而如果约束条件一般化为$g(x, y) = c$，那么$x$就不一定可以用其他变量表示出来了，这个时候就要利用**拉格朗日乘数法(Lagrange multipliers )**了。

*****************************************************************************

# 拉格朗日乘数法(Lagrange multipliers)
我们先一般化一个二元最优化问题为$(2.1)$形式：
$$
\min_{x, y} f(x, y)
$$
$$
s.t.　　g(x, y) = c
\tag{2.1}
$$
将目标函数$f(x, y)$和等式约束条件$g(x, y)=c$画出来就如下图所示：
![general_min][general_min]
其中的$f(x, y)$虚线为等高线，而红线为$g(x, y)=c$这个约束函数曲线与$f(x,y)$的交点的连线在$x-y平面$的映射。其中，假设有$d_3 > d_2 > d_1$， $d_1$点为最小值点（最优值点）。**从直观上可以发现，在$g(x,y)=c$与$f(x,y)$的非最优化交点A,B,C,D上，其$f(x,y)$和$g(x,y)$的法线方向并不是共线的，注意，这个相当关键，因为如果不是共线的，说明$g(x,y)=c$与$f(x,y)$的交点中，还存在可以取得更小值的点存在。对于A点来说，B点就是更为小的存在。因此，我们从直觉上推论出只有当$g(x,y)=c$与$f(x,y)$的法线共线时，才是最小值点的候选点（鞍点）。推论到多元变量的问题的时候，法线便用梯度表示**。于是，我们有原问题取得最优值的必要条件：
$$
\nabla f(x,y) = \nabla \lambda (g(x, y)-c)
\tag{2.2}
$$
$(2.2)$其中的$\lambda$表示两个梯度共线。
可以简单的变形为
$$
\nabla L(x, y, \lambda) = \nabla f(x,y) - \nabla \lambda (g(x, y)-c) = 0
\tag{2.3}
$$
让我们去掉梯度算子，得出
$$
L(x, y, \lambda) = f(x, y) - \lambda(g(x, y) - c)
\tag{2.4}
$$
看！我们得出了我们高数中经常见到的**等式约束下的拉格朗日乘数函数的表示方法**。

*****************************************************************************

# 广义拉格朗日乘数法(Generalized Lagrange multipliers)
上面我们的拉格朗日乘数法解决了等式约束的最优化问题，但是在**存在不等式约束的最优化问题**（包括我们SVM中需要求解的最优化问题）上，普通的拉格朗日乘数法并不能解决，因此学者提出了**广义拉格朗日乘数法（Generalized Lagrange multipliers）**， 用于解决含有不等式约束的最优化问题。这一章，我们谈一谈广义拉格朗日乘数法。
首先，我们先一般化我们的问题，规定一个二元标准的带有不等式约束的最小化问题(当然可以推广到多元的问题)，如：
$$
\min_{x, y} f(x, y)
$$
$$
s.t.　　g_i(x, y) \leq 0, i = 1,2,\cdots,N
$$
$$
h_i(x, y) = 0, i = 1,2, \cdots, N
\tag{3.1}
$$
类似于拉格朗日乘数法，参照式子$(2.4)$，我们使用$\alpha_i$和$\beta_i$作为等式约束和不等式约束的拉格朗日乘子，得出下式：
$$
L(x, y, \alpha, \beta) = f(x, y)+\sum_{i=1}^N \alpha_ig_i(x ,y)+\sum_{i=1}^N \beta_i h_i(x, y)
\tag{3.2}
$$
**KKT条件（Karush–Kuhn–Tucker conditions）**指出，当满足以下几个条件的时候，其解是问题最优解的候选解(摘自wikipedia)。

1. **Stationarity**
   * 对于最小化问题就是： $\nabla f(x,y)+\sum_{i=1}^N \alpha_i \nabla g_i(x ,y)+\sum_{i=1}^N \beta_i \nabla h_i(x, y) = 0 \tag{3.3}$
   * 对于最大化问题就是： $\nabla f(x,y)-(\sum_{i=1}^N \alpha_i \nabla g_i(x ,y)+\sum_{i=1}^N \beta_i \nabla h_i(x, y)) = 0 \tag{3.4}$

2. **Primal feasibility**
   * $g_i(x,y) \leq0, i = 1,2,\cdots,N \tag{3.5}$
   * $h_i(x, y) = 0, i = 1,2,\cdots,N \tag{3.6}$

3. **Dual feasibility**
   * $\alpha_i \geq 0, i = 1,2, \cdots, N \tag{3.7}$

4. **Complementary slackness**
   * $\alpha_i g_i(x, y) = 0, i = 1,2,\cdots,N \tag{3.8}$

其中的第一个条件和我们的拉格朗日乘数法的含义是相同的，就是梯度共线的意思；而第二个条件就是主要约束条件，自然是需要满足的；**有趣的和值得注意的是第三个和第四个条件，接下来我们探讨下这两个条件，以及为什么不等式约束会多出这两个条件。**

----
为了接下来的讨论方便，我们将N设为3，并且去掉等式约束，这样我们的最小化问题的广义拉格朗日函数就变成了：
$$
L(x, y, \alpha, \beta) = f(x, y)+\sum_{i=1}^3 \alpha_ig_i(x ,y)
\tag{3.9}
$$
绘制出来的示意图如下所示：
![general_lagrange_multipliers][general_lagrange_multipliers]
其中$d_i > d_j, when　i > j$，而蓝线为最优化寻路过程。
让我们仔细观察式子$(3.7)$和$(3.8)$，我们不难发现，因为$\alpha_i \geq 0$而$g_i(x, y) \leq 0$，并且需要满足$\alpha_i g_i(x, y) = 0$，所以$\alpha_i$和$g_i(x,y)$之中必有一个为0，那为什么会这样呢？

----

我们从上面的示意图入手理解并且记好公式$(3.3)$。让我们假设初始化一个点A， 这个点A明显不处于最优点，也不在可行域内，可知$g_2(x,y)>0$违背了$(3.5)$，为了满足约束$(3.8)$，有$\alpha_2=0$，导致$\alpha_2 \nabla g_2(x,y)=0$，而对于$i=1,3$，因为满足约束条件而且$g_1(x,y) \neq 0, g_3(x,y) \neq 0$，所以$\alpha_1 = 0, \alpha_3 = 0$。这样我们的式子$(3.3)$就只剩下$\nabla f(x,y)$，**因此对着$\nabla f(x,y)$进行优化，也就是沿着$f(x,y)$梯度方向下降即可，不需考虑其他的条件（因为还完全处于可行域之外）**。因此，A点一直走啊走，从A到B，从B到C，从C到D，这个时候因为D点满足$g_2(x,y)=0$，因此$\alpha_2 > 0$，所以$\alpha_2\nabla g_2(x,y) \neq 0$，因此$(3.3)$就变成了$\nabla f(x,y)+\alpha_2\nabla g_2(x,y)$所以在优化下一个点E的时候，就会考虑到需要满足约束$g_2(x,y) \leq 0$的条件，朝着向$g_2(x,y)$减小，而且$f(x,y)$减小的方向优化。因此下一个优化点就变成了E点，而不是G点。因此没有约束的情况下其优化路径可能是$A \rightarrow B \rightarrow C \rightarrow D \rightarrow G \rightarrow H$，而添加了约束之后，其路径变成了$A \rightarrow B \rightarrow C \rightarrow D \rightarrow E \rightarrow F$。

----

这就是为什么KKT条件引入了条件3和条件4，就是为了在满足不等式约束的情况下对目标函数进行优化。让我们记住这个条件，因为这个条件中某些$\alpha_i=0$的特殊性质，将会在SVM中广泛使用，而且正是这个性质定义了**支持向量(SV)**。



*****************************************************************************
# 引用
1. [拉格朗日乘子法如何理解？ 知乎][ref_1]
2. [《统计学习方法》 豆瓣][ref_2]
3. [《【直观详解】拉格朗日乘法和KKT条件》  微信公众号][ref_3]
4. [《解密SVM系列（一）：关于拉格朗日乘子法和KKT条件》 CSDN][ref_4]
5. [Karush–Kuhn–Tucker conditions wikipedia][ref_5]


[click]: https://github.com/FesianXu/AI_Blog/tree/master/SVM%E7%9B%B8%E5%85%B3
[ref_1]: https://www.zhihu.com/question/38586401
[ref_2]: https://book.douban.com/subject/10590856/
[ref_3]: http://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&mid=2247486230&idx=1&sn=41c3bcbf00b43535e912af1dab2704f5&chksm=ebb433c2dcc3bad45fa784314d4ea6a464538ff19b937b58bd99f12b43ec3c3cf01a269f9124&mpshare=1&scene=23&srcid=11211sArM7yh9Xf0pkvnWaJA#rd
[ref_4]: http://blog.csdn.net/on2way/article/details/47729419
[ref_5]: https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions


[general_min]: ./imgs/general_min.png
[general_lagrange_multipliers]: ./imgs/general_lagrange_multipliers.png


[^1]: 最高次数为1的多项式，形如 $f(x) = AX+B$，其中$X$是$m \times k$的仿射矩阵，其与线性函数的区别就是，线性函数是$f(x) = AX$没有偏置项$B$。