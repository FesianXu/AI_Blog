<div align='center'>
参数和非参数模型——当我谈到参数我在说些什么
</div>
<div align='right'>
FesianXu  2020/1/12
</div>

# 前言

参数模型和非参数模型是我们在机器学习中经常听到的术语，但是其中的“参数”却和我们通常理解的参数有所不同，本文尝试对此进行解释。如有谬误，请联系指出，谢谢。

$\nabla$ 联系方式：
e-mail: FesianXu@gmail.com
QQ: 973926198
github: https://github.com/FesianXu

----



# 对观察数据集进行描述

假如现在给我们观察数据$\mathcal{D} = \{\mathbf{X}_i, Y_i\}, i=0,\cdots,m$，其中$\mathbf{X} \in \mathbb{R}^{n}, Y \in \mathbb{R}$ 是表征这个观察数据的特征和标签，其中的$n$表示特征维度，$m$表示样本数量。 如果我们尝试对这个观察数据进行模型描述，我们可以怎么描述呢？把这个问题记住，我们继续探讨。

我们要认识到，对观察数据进行描述，指的不光光是把所有数据一个字节一个字节地“记住”（memorize），而是尝试用一个概率分布去描述这个观察数据，比如数据的联合概率分布$\mathrm{P}(\mathbf{X}, \mathrm{Y})$就可以很好地描述这个观察数据。为什么呢？比如说我们现在输入样本的特征是$X_1 = (0.1,0.2,0.2,0.5,0.1)$ 是一个5维向量，标签$\mathrm{Y} = 1$表征了其类别，那么概率
$$
\mathrm{P}(\mathbf{X}=X_1, \mathrm{Y}=1) = 0.1 \\\mathrm{P}(\mathbf{X}=X_1, \mathrm{Y}=0) = 0.3 \\\tag{1.1}
$$
这个概率表示了样本$X_1$和标签$Y=1$或者$Y = 0$同时出现的概率，通过计算边缘概率分布，我们同样知道了特征的概率分布：
$$
\mathrm{P}(\mathbf{X}) = \sum_{i} \mathrm{P}(\mathbf{X}, Y_i)\tag{1.2}
$$
我们在这里不用考虑(1.1)这个概率是怎么计算出来的（实际上这个正是模型所做的事），我们只要知道通过这种手段可以去表达观察数据集，我们把这个分布称之为“模型”（不太准确，但是可以这样理解）。从这个分布中进行采样我们足以生成虚拟的样本（生成模型的领域），当然这都是后话了。同样的，知道了这个分布，也足以解决我们的样本分类问题：
$$
\begin{aligned}\mathrm{P}(Y_j|\mathbf{X}) &= \dfrac{\mathrm{P}(\mathbf{X},Y_j)}{\mathrm{P}(\mathbf{X})} \\&= \dfrac{\mathrm{P}(\mathbf{X},Y_j)}{\sum_i\mathrm{P}(\mathbf{X}, Y_i)}\end{aligned}\tag{1.3}
$$
好的，那么我们现在的问题就集中在如何才能得到(1.1)的概率分布了，也就是怎么建模了。我们终于要进入正题了，哈哈哈哈。

总的来说，我们可以通过两种方法进行建模，一种称之为**参数化模型(parametric model)**，另一大类是**非参数模型(non-parametric model)**。注意，这里的“参数”和模型有没有可以学习的参数（比如神经网络中的weight）是没有关系的，非参数模型中可以有很多可学习的参数，但是不妨碍它为非参数模型。那么我们的问题就是怎么去理解这个“参数”了。

# 参数化模型

对(1.1)的概率分布进行建模，有一种最为直接的方法就是先假设这个分布是服从某个特定分布的，比如高斯分布，泊松分布等等，当然这些分布中有些未知参数需要我们求得，而这些参数也正是决定了这个分布的形状的，比如高斯分布的均值和协方差决定了不同的高斯分布，如下图所示。

![gaussian][gaussian]

<div align='center'>
    <b>
        Fig 1. 不同均值和协方差的高斯分布。
    </b>
</div>

我们也可以假设这个未知分布是多个已知分布的组合，比如多个高斯分布的组合，我们称之为**混合高斯模型（Gaussian Mixture Model,GMM）**，模型公式[1]如：
$$
p(x) = \sum_{k=1}^K p(k)p(x|k) = \sum_{k=1}^{K} \pi_k N(x|\mu_k, \Sigma_k)
\tag{2.1}
$$
其实就是K个不同均值和协方差的高斯分布的混合，并且对此进行了加权。

我们也可以假设我们的数据拟合曲线的形式，这个同样也是在隐式地对概率分布进行建模。经典的包括线性回归，逻辑斯蒂回归等，其函数形式都是如同：
$$
\begin{aligned}
y &= \theta_1 x_1 + \theta_2 x_2 \cdots + \theta_n \\
&= \Theta \mathbf{X} \\
& \Theta \in \mathbb{R}^{n}, \mathbf{X} = (x_1, x_2, \cdots, 1) \in \mathbb{R}^{n}
\end{aligned}
\tag{2.2}
$$
同样的，整个函数的形式都是已经确定了的，无非就是一个直线/超平面 而已，但是其具体的$\Theta$的组合，决定了这个超平面的具体走向。

这个就是所谓的参数化模型，我们需要根据经验，观察，专家知识等对数据分布进行一定的假设后，然后对决定这个分布形状的参数集$\Theta$进行求解，这个求解通常根据现有的观察到的数据集进行，这个参数集$\Theta$是一个有限的集合。

我们可以推出一个结论就是，在参数化模型的框架下，无论我接下来观察到多少数量的数据，哪怕是无限多个数据，我模型的参数量都只有固定数量多个，那便是$|\Theta|$ 。也就是说，用有界的参数量（复杂度）对无界的（数据量）的数据分布进行了建模。

假如你的假设分布足够靠谱，甚至是完全正确的，那么当你通过一些观察样本，得到了参数集$\Theta$之后，之后的预测结果将之和这个参数集有关，后续的任何观察样本$\mathcal{D}^{\prime}$都和预测结果无关，表示为：
$$
p(x|\Theta, \mathcal{D}^{\prime}) = p(x|\Theta)
\tag{2.3}
$$
显然这样模型并不是很灵活，模型的可靠性强依赖于对数据的人工分析经验等。



# 非参数化模型

非参数化模型，和参数化模型截然相反的是，对数据分布不进行任何的假设，只是依赖于观察数据，对其进行拟合。换句话说，其认为数据分布不能通过有限的参数集$\Theta$进行描述，但是可以通过无限维度的参数$\theta$进行描述，无限维度也就意味着其本质就是一个函数$f(\cdot) \in \mathbb{R}^{\infty}$。

通常，实际中的模型是对这个无限维度参数集的近似，比如神经网络中的参数，虽然参数量通常很大，也有万有拟合理论保证其可以拟合函数，但是其只是对无限维度数据的近似而已。由于非参数化模型依赖于观察数据，因此参数集$\theta$能捕获到的信息量随着观察数据集的数量增加而增加，这个使得模型更加灵活。



# 常见的模型归属

常见的参数化模型和非参数化模型有：

| 参数化模型        | 非参数化模型                                  | 应用场景     |
| ----------------- | --------------------------------------------- | ------------ |
| 多项式回归        | 高斯过程                                      | 函数近似     |
| 逻辑斯蒂回归      | 高斯过程分类器                                | 分类         |
| 混合模型，k-means | 狄利克雷过程混合(Dirichlet process mixtures)  | 聚类         |
| 隐马尔科夫模型    | 无限隐马尔科夫模型                            | 时间序列分析 |
| PCA/PMF           | 无限隐变量模型(infinite latent factor models) | 特征发掘     |
| ...               | ...                                           | ...          |

需要进行解释的是，神经网络可以看成是高斯过程的近似[2]，因此神经网络也是非参数化模型，k-means在聚类过程中假设数据是球型分布的（也就是欧式距离还管用，欧式距离可以表征样本之间的相似度的情况）。

# 这里指的参数到底是啥

所以这里谈到的参数到底是个啥呢？我认为，这里的参数与否其实指的是是否用参数对模型的形状进行了显式地描述，如有则是参数化模型，没有，那么就是非参数化模型了。

# Reference

[1].  https://blog.csdn.net/lin_limin/article/details/81048411 

[2].  Radford M. Neal. Priors for infinite networks (tech. rep. no. crg-tr-94-1). University of Toronto, 1994a. 



[gaussian]: ./imgs/gaussian.png