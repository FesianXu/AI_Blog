<h1 align = "center">正则化的原理和应用</h1>

## 前言
**在我们上一节曾经谈到过机器学习模型的容量，过拟合以及欠拟合等内容，也曾经提及控制容量的一种方法是控制其假设空间，事实上，通过在原模型的基础上添加正则化项的方式去控制模型的容量是更加理想的方式。这里，我结合《Deep Learning》中关于正则化项的讨论，浅谈一些所见所学。**
**如有谬误，请联系指正。转载请注明出处。**
*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`


----

# 没有免费午餐定理(No free lunch theorem)
　　我们常常说，某个机器学习算法优于某个机器学习算法，即是A优于B，**但是实际上真的存在这种区别吗？一个算法真的可能在所有数据生成分布上优于另一个吗？**答案是不！事实上，由Wolpert 1996证明了：
  
> 在所有的可能的数据生成分布$P_{data}$中，每一个机器学习分类算法在未事先观测的样本点上都有着相同的错   误率，换言之，在某种意义上，没有一种机器学习算法总是比其他的要好。我们能够设想到的最为优秀的算法和那些将所有点归为一类的“简单算法”有着**相同的平均性能**（既是在所有可能的任务上）。

　　**但是，我们的具体任务是符合某个特定数据生成分布的，因此，我们能够设计一个特定的算法可以足够描述特定任务的分布，尽管这个算法可能并不适用于其他的任务。**
  
# 正则化(Regularization)
　　没有免费午餐定理告诉我们在一个特定的任务上，我们必须专门设计一个适合的性能优良的机器学习算法，而且，我们可以给模型添加偏好(Preference)，当这些偏好潜在地和我们的任务吻合时，性能会更好。


## 偏好(Preference)
　　偏好是个有意思的概念。我们应该还记得模型的假设空间吧，一个假设空间那么大，很有可能会出现这么一种情况： **假设空间记为$D$，在$D$中可能存在多个点$\theta$，使得$\min_{\theta}f(x)$成立，其中$f(x)$是待优化的目标函数**。即是：
$$
  \min_{\theta}f(x), where　\theta \in D
$$
　　然而，这些多个“可行点”在**现有的训练集中表现出相同的性能，却并不意味着都可以有效地泛化**，我们需要设置一定的偏好去筛选适合的“可行点”，使得训练算法可以收敛到这个合适的点上，而不是其他的点，这便是偏好的作用。
  
  
  
## 权值衰减(Weight decay)
　　权值衰减是一种常见的偏好。例如，带权值衰减的线性回归最小化训练集上的均方误差(MSE)和正则项的和$J(w)$，其偏好于L2范式的平方较小的权重，如：
$$
J(w) = MSE_{train}+\lambda w^T w
$$
　　其中$\lambda$是调整偏好小L2范数程度的控制系数。当$\lambda=0$时，没有任何偏好，采用的还是基础的线性回归模型。越大的$\lambda$偏好其L2范式越小的权重，当$\lambda$太大时，将会没有任何斜率，既是变成了一个平行与某个坐标轴的直线。
![preference][preference]
　　偏好各种各样，有些可以控制参数的稀疏程度，这些偏好都是很重要的。

## 所以正则化是
　　谈了那么多，这里谈到的不同的方法显式地或隐式地表示对不同解的偏好都被称为正则化，正则化是指我们修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域的中心问题之一，只有优化能够与其重要性相媲。


# 常见的一些正则化




****



---------------------------------------未完待续-------------------------------------




[preference]: ./imgs/preference.png








