<div align='center'>
    漫谈视频理解
</div>

<div align='right'>
    2020/4/12 FesianXu
</div>

# 前言

AI算法已经渗入到了我们生活的方方面面，无论是购物推荐，广告推送，搜索引擎还是多媒体影音娱乐，都有AI算法的影子。作为多媒体中重要的信息载体，视频的地位可以说是数一数二的，然而目前对于AI算法在视频上的应用还不够成熟，理解视频内容仍然是一个重要的问题亟待解决攻克。本文对视频理解进行一些讨论，虽然只是笔者对互联网的一些意见的汇总和漫谈，有些内容是笔者自己的学习所得，希望还是能对诸位读者有所帮助。**如有谬误，请联系指出，转载请注明出处。**

$\nabla$联系方式：
**e-mail**: [FesianXu@gmail.com](mailto:FesianXu@gmail.com)
**QQ**: 973926198
github: https://github.com/FesianXu

------



# 为什么是视频

**以视频为代表的动态多媒体，结合了音频，视频，是当前的，更是未来的互联网流量之王**。 根据来自于国家互联网信息办公室的[中国互联网络发展状况统计报告](http://www.cac.gov.cn/wxb_pdf/0228043.pdf)[1]

> 截至 2018 年 12 月，网络视频、网络音乐和网络游戏的用户规模分别为 6.12 亿、5.76 亿和 4.84 亿，使用率分别为 73.9%、69.5%和 58.4%。短视频用户规模达 6.48 亿，网民使用比例为 78.2%。截至 2018 年 12 月，网络视频用户规模达 6.12 亿，较 2017 年底增加 3309 万，占网民 整体的 73.9%；手机网络视频用户规模达 5.90 亿，较 2017 年底增加 4101 万，占手机 网民的 72.2%。

其中各类应用使用时长占比如图Fig 1.1所示：

![apps_pros][apps_pros]

<div align='center'>
    <b>
    Fig 1.1 2018年各类应用使用时长占比。
    <b/>
</div>

我们很容易发现，包括短视频在内的视频用户时长占据了约20%的用户时长，占据了绝大多数的流量，同时网络视频用户的规模也在逐年增加。

互联网早已不是我们20年前滴滴答答拨号上网时期的互联网了，互联网的接入速率与日俱增，如Fig 1.2所示。视频作为与人类日常感知最为接近的方式，比起单独的图片和音频，文本，能在单位时间内传递更多的信息，从而有着更广阔的用户黏性。在单位流量日渐便宜，并且速度逐渐提升的时代，视频，将会撑起未来媒体的大旗。

![download_rate][download_rate]

<div align='center'>
    <b>
        Fig 1.2 固定带宽/4G平均下载速率变化曲线
</div>

的确，我们现在是不缺视频的时代，我们有的是数据，大型公司有着广大的用户基础，每天产生着海量的数据，这些海量的数据，然而是可以产生非常巨大的价值的。以色列历史学家尤瓦尔·赫拉利在其畅销书《未来简史》和《今日简史》中，描述过一种未来的社会，在那个社会中，数据是虚拟的黄金，垄断着数据的公司成为未来的umbrella公司，控制着人们的一言一行，人们最终成为了数据和算法的奴隶。尽管这个描述过于骇人和科幻，然而这些都并不是空穴来风，我们能知道的是，从数据中，我们的确可以做到很多事情，我们可以通过数据进行用户画像描写，知道某个用户的各个属性信息，知道他或她喜爱什么，憎恶什么，去过何处，欲往何方。我们根据用户画像进行精确的广告推送，让基于大数据的算法在无形中控制你的购物习惯也不是不可能的事情。数据的确非常重要，然而可惜的是，目前AI算法在视频——这个未来媒体之王上的表现尚不是非常理想，仍有很多问题亟待解决，然而未来可期，我们可以预想到，在视频上，我们终能成就一番事业。



# 理解视频——嗯，很复杂

利用视频数据的最终目标是让算法理解视频。**理解视频(understanding the video)**是一件非常抽象的事情，在神经科学尚没有完全清晰的现在，如果按照人类感知去理解这个词，我们终将陷入泥淖。我们得具体点，在理解视频这个任务中，我们到底在做什么？首先，我们要知道对比于文本，图片和音频，视频有什么特点。视频它是动态的按照时间排序的图片序列，然而图片帧间有着密切的联系，存在上下文联系；视频它有音频信息。因此进行视频理解，我们势必需要进行时间序列上的建模，同时还需要空间上的关系组织。

就目前来说，理解视频有着诸多具体的子任务：

1. 视频动作分类：对视频中的动作进行分类

2. 视频动作定位：识别原始视频中某个动作的开始帧和结束帧

3. 视频场景识别：对视频中的场景进行分类

4. 原子动作提取

5. 视频文字说明（Video Caption）：给给定视频配上文字说明，常用于视频简介自动生成和跨媒体检索

6. 集群动作理解：对某个集体活动进行动作分类，常见的包括排球，篮球场景等，可用于集体动作中关键动作，高亮动作的捕获。

7. 视频编辑。

8. 视频问答系统（Video QA）：给定一个问题，系统根据给定的视频片段自动回答

9. 视频跟踪：跟踪视频中的某个物体运动轨迹

10. 视频事件理解：不同于动作，动作是一个更为短时间的活动，而事件可能会涉及到更长的时间依赖

    ...

当然理解视频不仅仅是以上列出的几种，这些任务在我们生活中都能方方面面有所体现，就目前而言，理解视频可以看成是解决以上提到的种种问题。

通常来说，目前的理解视频主要集中在以人为中心的角度进行的，又因为视频本身是动态的，因此描述视频中的物体随着时间变化，在进行什么动作是一个很重要的任务，可以认为动作识别在视频理解中占据了一个很重要的地位。因此本文的理解视频将会和视频动作理解大致地等价起来，这样可能未免过于粗略，不过还是能提供一些讨论的地方的。

视频分析的主要难点集中在：

1. 需要大量的算力，视频的大小远大于图片数据，需要更大的算力进行计算。
2. 低质量，很多真实视频拍摄时有着较大的运动模糊，遮挡，分辨率低下，或者光照不良等问题，容易对模型造成较大的干扰。
3. 需要大量的数据标签！特别是在深度学习中，对视频的时序信息建模需要海量的训练数据才能进行。时间轴不仅仅是添加了一个维度那么简单，其对比图片数据带来了时序分析，因果分析等问题。



# 视频动作理解

## 视频数据模态

然而视频动作理解也是一个非常广阔的研究领域，我们输入的视频形式也不一定是我们常见的RGB视频，还可能是depth深度图序列，Skeleton关节点信息，IR红外光谱等。

![multiple_video_modality][multiple_video_modality]

<div align='center'>
    <b>
        Fig 3.1 多种模态的视频形式
</div>

就目前而言，RGB视频是最为易得的模态，然而随着很多深度摄像头的流行，深度图序列和骨骼点序列的获得也变得容易起来[2]。深度图和骨骼点序列对比RGB视频来说，其对光照的敏感性较低，数据冗余较低，有着许多优点。

关于骨骼点序列的采集可以参考以前的博文[2]。我们在本文讨论的比较多的还是基于RGB视频模态的算法。

## 视频动作分类数据集

现在公开的视频动作分类数据集有很多，比较流行的in-wild数据集主要是在YouTube上采集到的，包括以下的几个。

- UCF101，有着101个动作类别，13320个视频片段，大尺度的摄像头姿态变化，光照变化，视角变化和背景变化。

  ![ucf101][ucf101]

- sport-1M，也是在YouTube上采集的，有着1,133,157 个视频，487个运动标签。

  ![sports1m][sports1m]

- YouTube-8M, 有着6.1M个视频，3862个机器自动生成的视频标签，平均一个视频有着三个标签。

- YouTube-8M Segments[3]，是YouTube-8M的扩展，其任务可以用在视频动作定位，分段（Segment，寻找某个动作的发生点和终止点），其中有237K个人工确认过的分段标签，共有1000个动作类别，平均每个视频有5个分段。该数据集鼓励研究者利用大量的带噪音的视频级别的标签的训练集数据去训练模型，以进行动作时间段定位。

  ![youtube8M][youtube8M]

以上的数据集模态都是RGB视频，还有些数据集是多模态的：

- NTU RGB+D 60： 包含有60个动作，多个视角，共有约50k个样本片段，视频模态有RGB视频，深度图序列，骨骼点信息，红外图序列等。
- NTU RGB+D 120：是NTU RGB+D 60的扩展，共有120个动作，包含有多个人-人交互，人-物交互动作，共有约110k个样本，同样是多模态的数据集。

## 在深度学习之前

在深度学习之前，AI算法工程师是特征工程师，我们手动设计特征，而这是一个非常困难的事情。手动设计的特征主要套路有：

1. 局部特征（Local features）：比如HOG（梯度直方图 ）+ HOF（光流直方图）
2. 基于轨迹的（Trajectory-based）：Motion Boundary Histograms（MBH）[4]，improved Dense Trajectories （iDT） ——有着良好的表现，不过计算复杂度过高。
3. 





# Reference

[1]. http://www.cac.gov.cn/wxb_pdf/0228043.pdf

[2]. https://blog.csdn.net/LoseInVain/article/details/87901764

[3]. https://research.google.com/youtube8m/

[4]. Wang H, Kläser A, Schmid C, et al. Dense trajectories and motion boundary descriptors for action recognition[J]. International journal of computer vision, 2013, 103(1): 60-79.



[apps_pros]: ./imgs/apps_pros.jpg
[download_rate]: ./imgs/download_rate.jpg
[multiple_video_modality]: ./imgs/multiple_video_modality.jpg
[ucf101]: ./imgs/ucf101.jpg

[sports1m]: ./imgs/sports1m.jpg

[youtube8M]: ./imgs/youtube8M.jpg