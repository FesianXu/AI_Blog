<h1 align = "center">从贝叶斯的角度看曲线拟合问题以及对L2正则化的贝叶斯解释</h1>

## 前言

**在以前文章中，我们讨论过[《概率学派和贝叶斯学派的区别》](https://blog.csdn.net/loseinvain/article/details/80499147)和[《 <机器学习系列> 线性回归模型》](https://blog.csdn.net/loseinvain/article/details/78245665)，现在来看很多曲线拟合模型都是基于频率学派的方法进行学习的，这里简单介绍下基于贝叶斯学派的曲线拟合问题。**

**如有谬误，请联系指正。转载请注明出处。**

*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`
**code**: 

*******************************************************

# 贝叶斯曲线拟合
这里的曲线指的是多项式曲线（polynomial curve）[^1]，如下图所示：

![poly_curve][poly_curve]

一般来说，概率学派按照最小化平方和误差函数，如下所示，来进行参数的学习的。
$$
\mathcal{T}_{\theta} = \arg \min_{\omega} \mathcal{L}(\hat{y},y) \\
\hat{y}_j = \sum_{i=0}^N \omega_i x_{(i,j)}^{i} = y(x;\omega)\\
\mathcal{L}(\hat{y}, y) = \dfrac{1}{2}||\hat{y}-y||^2
\tag{1.1}
$$
$x_{(i,j)}$表示第$j$个样本的第$i$维数据值。
为了提高其泛化性能，一般还会进行L2正则处理，因此有正则损失函数为：
$$
\mathcal{L}_{\rm{total}}(\hat{y},y)=\frac{1}{2}||\hat{y}-y||^2+\frac{\lambda}{2}\omega^T\omega
\tag{1.2 正则后总损失}
$$


****

假设我们的数据具有高斯噪声，那么我们的预测就不是一个数值，，而是一个分布(如下图蓝线所示)，一般来说我们将其假设为是一个均值为$t$（也就是预测目标值），方差为$\sigma^2$的高斯分布（$\beta=\dfrac{1}{\sigma^2}$，$\beta$称之为精确度precision，表示了噪声对数据的影响），因此预测出来的分布如下式所示：
$$
p(t|x, \omega, \beta) = \mathcal{N} (t|y(x, \omega), \beta^{-1})
\tag{1.2}
$$
因为数据加了噪声的缘故，我们对数据的拟合具有了一定的不确定性，表示这种不确定性最好的工具就是用概率描述，因此我们的预测不是一个数值，而是一个分布。

图像看起就更加直观了：

![bayesian][bayesian]

可以看出，对于某一个预测，其为一个分布（蓝色线），其中预测的均值的预期就是观察值点A，可以看出，参数$\beta$决定了其置信范围$2\sigma$的大小。我们接下来用训练集$\{\mathbf{x}, \mathbf{t}\}$对式子(1.2)中的未知参数$\omega$和$\beta$进行参数估计。

如果这里采用频率学派中的观点进行点估计，那么就会采用**极大似然法**进行参数估计。似然函数如下所示：
$$
p(\textbf{t}|\textbf{x},\omega, \beta) = \prod_{i=0}^N \mathcal{N} (t_n | y(x_n, \omega), \beta^{-1})
\tag{1.3}
$$
为了计算方便转化为对数似然后，有：
$$
\mathcal{L} = \ln p(\textbf{t}|\textbf{x},\omega, \beta) = -\dfrac{\beta}{2} \sum_{n=1}^N \{y(x_n, \omega)-t_n\}^2 + \dfrac{N}{2}\ln \beta - \dfrac{N}{2} \ln (2\pi)
\tag{1.4 对数似然函数}
$$
我们可以从(1.4)舍去后面两项，因为其和关于$\omega$参数的最大化无关，同时因为最大值点与$\beta$的正倍数也无关，因此可以取$\beta=1$，于是我们有关于$\omega$的对数似然为：
$$
\mathcal{L} = \ln p(\textbf{t}|\textbf{x},\omega, 1) = -\dfrac{1}{2} \sum_{n=1}^N \{y(x_n, \omega)-t_n\}^2 
\tag{1.5 关于$\omega$的对数似然函数}
$$
最大化对数似然相当于最小化负对数似然，那么有：
$$
\min_{\omega} -\mathcal{L} = \dfrac{1}{2} \sum_{n=1}^N \{y(x_n, \omega)-t_n\}^2 
\tag{1.6 负对数似然}
$$
而这，刚好就是平方和损失（也称之为mean-square error，MSE），所以，**在曲线拟合问题中，平方和损失是数据满足高斯噪声的情况下引入的。**

----




# Reference
1. Bishop 《Pattern Recognize and Machine Learning, PRML》
2. [《概率学派和贝叶斯学派的区别》](https://blog.csdn.net/loseinvain/article/details/80499147)
3. [《 <机器学习系列> 线性回归模型》](https://blog.csdn.net/loseinvain/article/details/78245665)





[^1]: A curve obtained by fitting polynomials to each ordinate of an ordered sequence of points. 指的是用多项式函数$f(\textbf{X}; \theta)=\sum_{i=0}^N \theta_i x_i^{i}, \textbf{X} \in \mathbb{R}^N$。其中如果指数全部变为1而不是$i$，则退化为线性回归。


[poly_curve]: ./imgs/poly_curve.png
[bayesian]: ./imgs/bayesian.png


