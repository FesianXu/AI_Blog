<h1 align = "center">贝叶斯之旅||第二讲，分类问题的两大过程，推理和决策</h1>

## 前言

**前面[1]我们介绍了贝叶斯决策的一些知识，介绍了基于最小化分类错误率和最小化分类损失的两种决策准则，接下来，我们简单讨论下分类问题中的二个步骤，推理和决策。**

**如有谬误，请联系指正。转载请注明出处。**

*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`

*******************************************************

# 分类问题
我们在之前的文章中已经介绍过分类问题了，简单的说就是给定一个样本$\bf{x} \in \mathbb{R}^n$，将其划分到有限的标签集$\bf{Y} \in \{0,1,\cdots,m\}$中。通常来说，我们可以将整个分类问题划分为两个独立的过程，分别是**推理（inference）**和**决策(decision)**阶段。在推理阶段，我们通过已有的训练集，学习到后验概率$p(\mathcal{C}_k|\bf{x})$，或者也可以通过学习联合概率分布$p(\mathcal{C}_k, \bf{x})$，然后也可以得到后验概率。而接下来，在决策阶段，就根据这个后验概率，对样本的类别进行判断决策。这个决策过程可以参考文章[1]的讨论。

**注意到，很多时候，这两个过程可以合在一起，将问题简化为成：学习一个映射$f(\bf{x}) \in \mathbb{R}^m, \bf{x} \in \mathbb{R}^n$，直接将样本映射到类别标签。**这个过程中，将不会涉及到任何的后验概率等，而是直接得出预测结果，这个函数因此称之为**判别函数(Discriminant function)**。[2] page 43

事实上，这些讨论过的方法都可以用来解决分类问题，并且在实际应用中都有所应用，我们按照复杂程度进行降序排列之后，有：

1. 通过解决推理问题之后，我们可以给每一个类别估计出类条件概率$p(\mathbf{x}|\mathcal{C}_k)$，同时，先验概率$p(\mathcal{C}_k)$也很容易可以估计出来，然后通过贝叶斯公式我们可以得到后验概率：
$$
p(\mathcal{C}_k | \mathbf{x}) = \frac{p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{p(\mathbf{x})}
\tag{1.1}
$$
我们有:
$$
p(\mathbf{x}) = \sum_{k} p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)
\tag{1.2 对输入分布进行建模}
$$
等价地，我们可以对联合概率密度$p(\mathbf{x},\mathcal{C}_k)$进行建模，然后进行标准化后得到后验概率。像这种显式地或者隐式地对输入和输出进行概率分布建模的模型，称之为**生成模型(generative models)**，因为从这个联合分布中进行采样可以生成输入空间中的一些**虚假生成数据(synthetic data)**。

2. 通过解决推理问题后，得到后验概率$p(\mathcal{C}_k|\mathbf{x})$，然后通过决策论进行类别判断。这种模型称之为**判别模型(Discriminative model)**。
3. 寻找一个函数$f(\mathbf{x})$，称之为判别函数，直接将输入的$\mathbf{x}$映射到一个类别标签上，比如SVM分类器等。在这个情形下，并没有用到任何概率，也就是说我们对预测的结果其实是没有办法判断可靠程度的。

我们接下来分别讨论下这三种方法的优劣点。

-----

# 孰优孰劣，判别模型和生成模型
## 生成模型
生成模型是对于数据量需求最高的，同时运算量也是最大的，因为其需要训练出包含$\mathbf{x}$和$\mathcal{C}_k$的联合分布，如果数据量不够，将会导致严重的过拟合现象[3]。对于很多应用下来说，$\mathbf{x}$是一个维度很高的特征向量，因此为了使得类条件概率得到一个较为合理的精度，就需要很多的数据量进行计算。但是，生成模型也有一些很好的性质，比如说可以从中进行采样生成出一些假数据，这个应用目前在很多image inpainting[4]，style transfer[5]任务中经常用到。而且，因为通过联合概率分布可以通过式子(1.2)计算出边缘概率分布$p(\mathbf{x})$。这个输入空间的边缘概率分布很有用，因为其可以判断输入的新数据是否是一个所谓的**离群点(outlier)**，离群点如下图所示。这个就是所谓的**离群点检测(outlier detection)**或者称之为**异常检测(novelty detection)**，这个在网络欺诈预测，银行欺诈预测，电子垃圾邮件检测中很有用。

![outlier][outlier]

## 判别模型
在分类任务中，很多时候你只是做个分类而已，并不用进行离群点检测，也不需要生成虚假样本.这个时候，如果还用生成模型去进行后验概率的估计，就浪费了很多资源。我们观察下图，我们可以发现，类条件概率其实和后验概率并没有必然的影响。这个时候，你就需要采用判别模型。
![posterior_class][posterior_class]
不仅如此，采用了判别模型还有一个好处就是，可以利用所谓的**拒绝域(reject option)**把一些过于边缘的判断拒绝掉。比如我们仅有10%的把握判断某人为癌症患者，那么我们就情愿不做这个判断，交给更为权威的人或者系统进行下一步的处理。如下图所示，绿色的水平线表示拒绝水平，只有后验概率高于这个水平线，才能认为是可靠的判断。我们将会看到，在基于判别函数的情况下，因为并没有概率的存在，因此并不能进行这种操作。

![reject][reject]

## 判别函数方法
有比以上俩种方法更为简单，计算量更少的方法，那就是判别函数法。在这个情况下，因为是直接用训练数据拟合一个函数$f(\mathbf{x})$对样本进行分类，因此无法得到后验概率$p(\mathcal{C}_k|\mathbf{x})$。在这个方法中，只能最小化分类错误率，而没法给不同类型的分类错误进行区别[1]，采用最小化分类风险，这是个遗憾的地方。

----

# Reference
[1] [《贝叶斯之旅||第一讲，贝叶斯决策》](https://blog.csdn.net/LoseInVain/article/details/82780472)

[2] Bishop C M. Pattern recognition and machine learning (information science and statistics) springer-verlag new york[J]. Inc. Secaucus, NJ, USA, 2006.

[3] [《机器学习模型的容量，过拟合与欠拟合》](https://blog.csdn.net/LoseInVain/article/details/78108990)

[4] [《基于深度学习的Image Inpainting (图像修复)论文推荐(持续更新)》](https://blog.csdn.net/gavinmiaoc/article/details/80802967)

[5] [《Image Style Transfer》](https://www.jianshu.com/p/b1189448eb2e)


[posterior_class]: ./imgs/posterior_class.png
[outlier]: ./imgs/outlier.jpg
[reject]: ./imgs/reject.png