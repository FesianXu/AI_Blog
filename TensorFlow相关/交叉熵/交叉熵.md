<h1 align = "center">TensorFlow中的交叉熵损失函数</h1>


## 前言

**如有谬误，请联系指正。转载请注明出处。**
*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`

****

# tf.nn.softmax_cross_entropy_with_logits()
原型为：
```python
 softmax_cross_entropy_with_logits(
    _sentinel=None,
    labels=None,
    logits=None,
    dim=-1,
    name=None
)
```
其中`logits`为经过线性网络的得分，没有经过softmax处理，因为TensorFlow内部会进行softmax处理，比用户自己处理速度更快。`labels`是标签，是督导数据。
其运算公式为：
$$
f(logits, labels) = \sum_{i \in D } labels \bigodot log(softmax(logits)), 其中D为维数
$$

```python
    target = tf.Variable([0, 1, 0])
    logit = tf.Variable([0.7,0.2,0.1])
    op = tf.nn.softmax_cross_entropy_with_logits(logits=logit, labels=target)
    so = tf.nn.softmax(logit)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        print(so.eval())
        print(op.eval())
```

输出为:
```python
output: [ 0.46396342  0.28140804  0.25462851]
output: 1.26795
```
可以看出，其答案为：
$$
\sum [0, 1, 0] \bigodot [ 0.46396342，　0.28140804，　0.25462851] = 1.26795
$$


