# 神经元函数

tf.nn.relu()
tf.nn.sigmoid()
tf.nn.tanh()
tf.nn.elu()
tf.nn.bias_add()
tf.nn.crelu()
tf.nn.relu6()
tf.nn.softplus()
tf.nn.softsign()
tf.nn.dropout() # 防止过拟合，用来舍弃某些神经元

## tf.nn.dropout()
　　dropout 函数。一个神经元将以概率 keep_prob 决定是否被抑制。如果被抑制，该神经
元的输出就为 0；如果不被抑制，那么该神经元的输出值将被放大到原来的 1/keep_prob 倍。
　　在默认情况下，每个神经元是否被抑制是相互独立的。但是否被抑制也可以通过 noise_shape 来调节。当 noise_shape[i] == shape(x)[i]时， x 中的元素是相互独立的。如果 shape(x) = [k, l, m, n]，x 中的维度的顺序分别为批、行、列和通道，如果 noise_shape = [k, 1, 1, n]，那么每个批和通道
都是相互独立的，但是每行和每列的数据都是关联的，也就是说，要不都为 0，要不都还是原
来的值。

```python
a = tf.constant([[-1.0, 2.0, 3.0, 4.0]])
with tf.Session() as sess:
b = tf.nn.dropout(a, 0.5, noise_shape = [1,4])
print sess.run(b)
b = tf.nn.dropout(a, 0.5, noise_shape = [1,1])
print sess.run(b
```