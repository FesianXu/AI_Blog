<h1 align = "center">反向传播算法及其梯度扩散</h1>

## 前言
**最近开始认真学习了下反向传播算法和梯度传递的问题，其本质是导数的链式法则的应用，由此可以分析出为什么sigmoid激活函数不适合用于做多层网络的激活函数，可以考虑联系我的另一篇关于激活函数的文章。如有谬误，请联系指正。转载请注明出处。**
*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`


----
# 神经网络

要知道什么是反向传播算法，我们还是要从神经网络开始将起。神经网络如图所示。

![network][network]

****

图中所示的是一个具有一个输入层（input layer），一个隐藏层（hidden layer），一个输出层（output layer）的三层神经网络。我们都知道，在神经网络的训练过程中，其权值是通过梯度的变化大小来更新的，我们这里先进行符号规定，以便于接下来的分析。



$w^l_{jk}$ 是**l-1**层的第k个神经元与**l**层的第j个神经元的*连接权值*



$b^l_{j}$ 是**l-1**层的第j个神经元的偏置。


$z^l_{j}$ 是**l**层的第j神经元的输入。

$a^l_{j}$ 是**l**层的第j神经元的激活输出。



用公式表达既是：

$z^l_{j} = \Sigma_{k} (w^l_{jk}*a^{l-1}_{k}+b^l_j)$

$a^l_j = \sigma(z^l_j) = \sigma(\Sigma_k (w^l_{jk}*a^{l-1}_k+b^l_j) )$

其中的$\sigma(x)$为激活函数。多采用sigmoid，ReLU，tanh等。

$C = \frac{1}{2n}*\Sigma_x||y-a^L||^2$ 代价函数，其中L是神经网络的层数。

****



我们知道，进行权值更新的时候，我们采用的是梯度下降法更新(Gradient Descent)， 公式如下：

$$w^l_{jk} := w^l_{jk}-\eta*\nabla\frac{\partial{C}}{\partial{w^l_{jk}}}$$

这里最重要的便是需要求得$\nabla\frac{\partial{C}}{\partial{w^l_{jk}}}$，为了求得这个导数，我们现在有几个公式需要推导，这些公式大多都有链式法则推导而出。





<!--第一个公式-->
****
## $\delta^L = \nabla_a{C}\bigodot \sigma^\prime(z^L)$ （对L层的误差）



其中我们要注意的是，对于某层的误差$\delta^l$定义为$\delta^l = \frac{\partial{C}}{\partial{z^l}}$，其中具体到某一个神经元j的误差为$\delta^l_j = \frac{\partial{C}}{\partial{z^l_j}}$。

推导：

$$\delta^L_j = \frac{\partial{C}}{\partial{z^L_j}} = \frac{\partial{C}}{\partial{a^L_j}}*\frac{\partial{a^L_j}}{\partial{z^L_j}} = \nabla_a{C}*\sigma^\prime(z^L_j) $$

所以当扩展到对于所有的第L层的神经元时，我们为了方便用一个矩阵去代表我们的结果：

$$ \delta^L = \left(\begin{array}{c}

                        \delta^L_1 \\ 

                        \delta^L_2

			  \end{array}\right)$$

需要注意的是，所有的误差矩阵都可以这样定义，如：

$$ \delta^l = (\delta^l_1, \delta^l_2, \cdots,\delta^l_n)^T， 其中n是l层神经元的数量$$

类似的，可以得出：

$$ \sigma^\prime(z^l) = (\sigma^\prime(z^l_1),\sigma^\prime(z^l_2),\cdots,\sigma^\prime(z^l_n))，其中n是第l层的神经元数量$$

由此可以得出结论：

$$\delta^L = \nabla_a{C}\bigodot \sigma^\prime(z^L)$$





<!--第二个公式-->
*****
## $  \delta^l_j = \Sigma_k(\delta^{l+1}_k*w^{l+1}_{kj}*\sigma^\prime(z^l_j))  $ 第l层第j个神经元的误差



$$  
\delta^l_j = 
\frac{\partial{C}}{\partial{z^l_j}} = 
\Sigma_k \frac{\partial C}{ \partial z^{l+1}_k} * \frac{ \partial{z^{l+1}_k} }{ \partial{a^l_j} } * 
\frac{ \partial{a^l_j} }{ \partial{z^l_j} }  
=  \Sigma_k \delta^{l+1}_k * \frac{ \partial({w^{l+1}_{kj}} * a^l_j+b^{l+1}_k )}{ \partial{a^l_j} } * \sigma^\prime(z^l_j)
= \Sigma_k(\delta^{l+1}_k*w^{l+1}_{kj}*\sigma^\prime(z^l_j))
$$



同样的，为了表示方便，也多用矩阵形式表示，有：

$ \delta^l_1 = \delta^{l+1}_1*w^{l+1}_{11}*\sigma^\prime(z^l_1)+
\delta^{l+1}_2*w^{l+1}_{21}*\sigma^\prime(z^l_1)
$



$ \delta^l_2 = \delta^{l+1}_1*w^{l+1}_{12}*\sigma^\prime(z^l_2)+
\delta^{l+1}_2*w^{l+1}_{22}*\sigma^\prime(z^l_2)
$



$ \delta^l_3 = \delta^{l+1}_1*w^{l+1}_{13}*\sigma^\prime(z^l_3)+
\delta^{l+1}_2*w^{l+1}_{23}*\sigma^\prime(z^l_3)
$



$ \delta^l_4 = \delta^{l+1}_1*w^{l+1}_{14}*\sigma^\prime(z^l_4)+
\delta^{l+1}_2*w^{l+1}_{24}*\sigma^\prime(z^l_4)
$



$
W^{l+1} = \left(\begin{array}{ccc}
		w^{l+1}_{11} & w^{l+1}_{12} & w^{l+1}_{13}\\ 
		w^{l+1}_{21} & w^{l+1}_{22} & w^{l+1}_{23}
	\end{array}\right)
$



所以，有：

$$
\delta^l = ((W^{l+1})^T * \delta^{l+1}) \bigodot \sigma^\prime(z^l)
$$



<!--第三个公式-->
****
## $ \frac{\partial{C}}{\partial{w^l_{jk}}} = a^{l-1}_k*\delta^l_j$ 误差具体到对某个边权重的偏导数

$$ 
\frac{\partial{C}}{\partial{w^l_{jk}}} = \frac{\partial{C}}{\partial{z^l_{j}}} * 
\frac{\partial{z^l_j}}{\partial{w^l_{jk}}} = 
a^{l-1}_k*\delta^l_j=a^{l-1}_k * 
\Sigma_k(\delta^{l+1}_k*w^{l+1}_{kj}*\sigma^\prime(z^l_j))
$$

由此可以看出，$\frac{\partial{C}}{\partial{w^l_{jk}}}$与$\sigma^\prime(z^l_j)$呈正相关的关系，当激活函数采用了sigmoid函数时，由于x越大，其导数呈现指数衰减，所以在层数太大而且输出值范围太大的时候，
















[network]:./imgs/network.png