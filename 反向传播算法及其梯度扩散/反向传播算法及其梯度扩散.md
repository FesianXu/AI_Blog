<h1 align = "center">反向传播算法及其梯度扩散</h1>

# 神经网络
要知道什么是反向传播算法，我们还是要从神经网络开始将起。神经网络如图所示。
![network][network]
****
图中所示的是一个具有一个输入层（input layer），一个隐藏层（hidden layer），一个输出层（output layer）的三层神经网络。我们都知道，在神经网络的训练过程中，其权值是通过梯度的变化大小来更新的，我们这里先进行符号规定，以便于接下来的分析。

$w^l_{jk}$ 是**l-1**层的第k个神经元与**l**层的第j个神经元的*连接权值*

$b^l_{j}$ 是**l-1**层的第j个神经元的偏置。

$z^l_{j}$ 是**l**层的第j神经元的输入。

$a^l_{j}$ 是**l**层的第j神经元的激活输出。

用公式表达既是：
$z^l_{j} = \Sigma_{k} (w^l_{jk}*a^{l-1}_{k}+b^l_j)$

$a^l_j = \sigma(z^l_j) = \sigma(\Sigma_k (w^l_{jk}*a^{l-1}_k+b^l_j) )$
其中的$\sigma(x)$为激活函数。多采用sigmoid，ReLU，tanh等。

$C = \frac{1}{2n}*\Sigma_x||y-a^L||^2$ 代价函数，其中L是神经网络的层数。
****

我们知道，进行权值更新的时候，我们采用的是梯度下降法更新(Gradient Descent)， 公式如下：
$$w^l_{jk} := w^l_{jk}-\eta*\nabla\frac{\partial{C}}{\partial{w^l_{jk}}}$$
这里最重要的便是需要求得$\nabla\frac{\partial{C}}{\partial{w^l_{jk}}}$，为了求得这个导数，我们现在有几个公式需要推导，这些公式大多都有链式法则推导而出。


<!--第一个公式-->
## $\delta^L = \nabla_a{C}\bigodot \sigma^\prime(z^L)$ （对L层的误差）

其中我们要注意的是，对于某层的误差$\delta^l$定义为$\delta^l = \frac{\partial{C}}{\partial{z^l}}$，其中具体到某一个神经元j的误差为$\delta^l_j = \frac{\partial{C}}{\partial{z^l_j}}$。
推导：
$$\delta^L_j = \frac{\partial{C}}{\partial{z^L_j}} = \frac{\partial{C}}{\partial{a^L_j}}*\frac{\partial{a^L_j}}{\partial{z^L_j}} = \nabla_a{C}*\sigma^\prime(z^L_j) $$
所以当扩展到对于所有的第L层的神经元时，我们为了方便用一个矩阵去代表我们的结果：
$$ \delta^L = \left(\begin{array}{c}
                        \delta^L_1 \\ 
                        \delta^L_2
			  \end{array}\right)$$
需要注意的是，所有的误差矩阵都可以这样定义，如：
$$ \delta^l = (\delta^l_1, \delta^l_2, \cdots,\delta^l_n)^T， 其中n是l层神经元的数量$$
类似的，可以得出：
$$ \sigma^\prime(z^l) = (\sigma^\prime(z^l_1),\sigma^\prime(z^l_2),\cdots,\sigma^\prime(z^l_n))，其中n是第l层的神经元数量$$
由此可以得出结论：
$$\delta^L = \nabla_a{C}\bigodot \sigma^\prime(z^L)$$


<!--第二个公式-->
## $  \delta^l_j = \Sigma_k(\delta^{l+1}_k*w^{l+1}_{kj}*\sigma^\prime(z^l_j))  $ 第l层第j个神经元的误差

$$  
\delta^l_j = 
\frac{\partial{C}}{\partial{z^l_j}} = 
\Sigma_k \frac{\partial C}{ \partial z^{l+1}_k} * \frac{ \partial{z^{l+1}_k} }{ \partial{a^l_j} } * 
\frac{ \partial{a^l_j} }{ \partial{z^l_j} }  
=  \Sigma_k \delta^{l+1}_k * \frac{ \partial({w^{l+1}_{kj}} * a^l_j+b^{l+1}_k )}{ \partial{a^l_j} } * \sigma^\prime(z^l_j)
= \Sigma_k(\delta^{l+1}_k*w^{l+1}_{kj}*\sigma^\prime(z^l_j))
$$

同样的，为了表示方便，也多用矩阵形式表示，有：
$ \delta^l_1 = \delta^{l+1}_1*w^{l+1}_{11}*\sigma^\prime(z^l_1)+
\delta^{l+1}_2*w^{l+1}_{21}*\sigma^\prime(z^l_1)
$

$ \delta^l_2 = \delta^{l+1}_1*w^{l+1}_{12}*\sigma^\prime(z^l_2)+
\delta^{l+1}_2*w^{l+1}_{22}*\sigma^\prime(z^l_2)
$

$ \delta^l_3 = \delta^{l+1}_1*w^{l+1}_{13}*\sigma^\prime(z^l_3)+
\delta^{l+1}_2*w^{l+1}_{23}*\sigma^\prime(z^l_3)
$

$ \delta^l_4 = \delta^{l+1}_1*w^{l+1}_{14}*\sigma^\prime(z^l_4)+
\delta^{l+1}_2*w^{l+1}_{24}*\sigma^\prime(z^l_4)
$

$
W^{l+1} = \left(\begin{array}{ccc}
		w^{l+1}_{11} & w^{l+1}_{12} & w^{l+1}_{13}\\ 
		w^{l+1}_{21} & w^{l+1}_{22} & w^{l+1}_{23}
	\end{array}\right)
$

所以，有：
$$
\delta^l = ((W^{l+1})^T * \delta^{l+1}) \bigodot \sigma^\prime(z^l)
$$









[network]:./imgs/network.png