<h1 align = "center">机器学习性能评判标准</h1>

## 前言
**
衡量一个机器学习算法的优劣有许多指标可以参考，对于不同任务的不同算法需要采用不同的指标进行衡量而不能一概而论，正如没有免费午餐定律所揭示的，没有任何一个算法可以在所有数据集上表现良好，因此需要用合适的性能评判指标去选择合适的机器学习模型。这里浅谈一些评判指标，部分素材来自《机器学习》 周志华。
**

**如有谬误，请联系指正。转载请注明出处。**
*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`


----

# 采用不同学习器性能评判的必要性
　　正如前言所说，对于一个特定的任务需要一个特定的性能评判标准，那为什么不同任务的评判标准不能通用呢？比如说经常用的误差率$error=N_{wrong}/N_{total}$不是能很好的表示性能吗？为什么要多此一举呢？
　　让我们假设一个场景：
> 在医疗中，患癌症是一个小概率事件（先验概率小），而正常才是一个大概率事件，假设一个区域的人经过统计可以知道患癌的人和不患癌的人比例为1：99。

　　那么如果单纯将任何一个新的病人（未知其患病情况）都看成是正常的，那么如果按照误差率来说只有1%，然而，这样就失去预测的意义。因此，**误差率**或者说是**准确率**对于这种高度不平衡的样本的任务不适合，需要用更加适合的评判标准进行评估。
  
# 不同的性能评估指标
![Evaluation]

# 分类问题
　　在分类问题中，预测的是一个类的标签，有离散的性质。

## 混淆矩阵(confusion matrix)
　　混淆矩阵在分类问题中，特别是多分类问题中应用广泛，其一般形式如：
![ConfusionMatrix]  
其中对角线的表示正确分类的，其余的都是错分类的。当只有两类的时候，退化为：
![BinaryConfusionMatrix]
其中，**FP为误报样本**，**FN为漏报样本**。

## Accuracy 准确率
　　就是一开始提到的误差率的补数，为：
$$
Accuracy = \frac{TP+TN}{TP+TN+FP+FN}　 (Binary　Class)
$$
$$
Accuracy = Tri(M)/\sum_i\sum_jM_{ij}　 (Multiple　Class)
$$
　　不适合用于不平衡的先验分布的数据集，如患病人群，网络攻击等。
  
## Precision 精确率，查准率
　　Precision是针对预测结果而言的，表示预测为某一个类的样本有多少是正确的，也用于评估**检索出的信息中有多少是用户感兴趣的**，也被称为查准率。
$$
Precision = \frac{TP}{TP+FP}　(Binary　Class)
$$
$$
Precision(j)_N = \frac{M_{j, j}}{\sum_{i=1}^N M_{i,j}}　(Multiple　Class)
$$

## Recall 召回率，查全率
　　Recall对于原先的样本而言的，表示样本中的正例有多少被预测正确了，也用于评估**用户感兴趣的信息中有多少被检索出来了。**
$$
recall = \frac{TP}{TP+FN}　(Binary　Class)
$$
$$
Precision(i)_N = \frac{M_{i, i}}{\sum_{j=1}^N M_{i,j}}　(Multiple　Class)
$$

## F1 measure
　　通常来说，Precision和Recall是此消彼长的关系，而这两者都描述了模型的性能，因此可以取两者的调和平均数，称为F1测量值。当P和R都高的时候，F1也会高。
$$
\frac{2}{F_1} = \frac{1}{Precision}+\frac{1}{Recall}
$$

## ROC曲线
　　ROC曲线多用于二类分类器的评估，要理解**ROC曲线**(Receiver Operating Characteristic curve)，我们需要先理解什么叫做**决策阀值**，比如在逻辑斯蒂回归中，我们会设置一个阀值t，其本质为超参数，其分类结果为：
$$
\begin{cases}
1 & \sigma(w^Tx+b) > t \\
-1  & \sigma(w^Tx+b) \leq t
\end{cases}
$$
　　**一般我们会将t设为0**，但是t作为超参数也是可以自由设定的，**如果减少t，更多样本被识别为正，提高了正类识别率，但是也使得更多的负类被错误分为正类**。而ROC曲线正是描述了这一性能，关注两个指标：
$$
True Positive Rate, TPR=\frac{TP}{TP+FN}
$$
$$
False Positive Rate, FPR=\frac{FP}{FP+TN}
$$
TPR表示将正例分对的概率，FPR表示将负类错分为正例的概率。
![ROC]
　　我们看到ROC曲线都在$y=x$的曲线之上，因为$y=x$表示了随即猜测，**不存在比随机猜测更糟糕的机器学习算法，因为总是可以将错误率转换为正确率，如一个算法的正确率为40%，那么将两类的标签互换，正确率就变为了60%**。最理想的分类器是到达(0,1)点的折线，现实中不存在。**如果我们说一个机器学习算法A比B好，那么我们指的是A的ROC曲线完全覆盖了B的ROC曲线，如果有交点，只能说明A在某个场合优于B。**，如下图：
![ROC_better]
这个代表了红色分类器的性能优于蓝色的。
![ROC_conditional_better]
而这个只能表示红色分类器在某些场景优于其他的。


## AUC(Area Uder ROC Curve)
　　AUC被定义为ROC曲线下的面积，AUC肯定大于0.5，AUC越大的分类器，其平均正确率越高。
$$
\begin{cases}
完美分类器 & AUC = 1 \\
优于随即猜测，需要设定阀值，有预测价值的分类器  & 0.5 < AUC < 1.0 \\
不存在  & AUC < 0.5 \\
随机猜测 & AUC = 0.5
\end{cases}
$$


# 回归问题
　　在回归问题中，预测值通常为连续值，其性能不能用常规的分类问题的评估指标表示。一般我们用：
* 平均绝对误差 MAE，即是$L_1$范数损失。
$$
MAE(y, \hat{y}) = \frac{1}{N_{samples}}\sum_{i=1}^{N_{samples}}|y_i-\hat{y_i}|
$$
* 平均平方误差 MSE， 即是$L_2$范数损失。
$$
MSE(y, \hat{y}) = \frac{1}{N_{samples}}\sum_{i=1}^{N_{samples}}||y_i-\hat{y_i}||^2
$$


[Evaluation]: ./imgs/Evaluation.png
[ConfusionMatrix]: ./imgs/ConfusionMatrix.png
[BinaryConfusionMatrix]: ./imgs/BinaryConfusionMatrix.png
[ROC]: ./imgs/ROC.jpg
[ROC_better]: ./imgs/ROC_better.jpg
[ROC_conditional_better]: ./imgs/ROC_conditional_better.jpg
