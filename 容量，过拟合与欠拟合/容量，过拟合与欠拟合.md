<h1 align = "center">容量，过拟合与欠拟合</h1>

## 前言
**过拟合与欠拟合是机器学习中永恒的话题，而过拟合这个问题也是机器学习中经常导致模型失败的罪魁祸首，这里结合《Deep Learning》一书中的关于容量，过拟合与欠拟合的讨论，总结,摘抄一些观点。**
**如有谬误，请联系指正。转载请注明出处。**
*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`


----

# 泛化问题(generalization)
　　机器学习的主要挑战在于**在未见过的数据输入上表现良好**，这个能力称为**泛化能力**（generalization）。而我们的机器学习模型都是从训练集中学习参数得到的，如何确保其在和训练集“隔离”的测试集中表现良好呢？
　　这里我们量化一下模型在训练集和测试集上的表现，将其分别称为**训练误差**(training error)和**测试误差**(test error)，后者也经常称为**泛化误差**(generalization error)。可以说，理想的模型就是在最小化训练误差的同时，最小化泛化误差。
　　如果我们的训练集和测试集都是随机生成的，不服从任何分布，那么这个模型将会注定在测试集上表现糟糕，事实上，将不会有任何模型可能在其上表现地明显优于随机猜测。因此我们这里假设我们的训练集和测试集都是从一个**独立同分布**(i.i.d assumption)的概率分布中生成的，意思即是，训练集和测试集中的每个样本之间都是**独立**的，同时每个样本由同一个**分布**生成。这样就给模型的泛化提供了基本假设。我们将这个潜在的分布称为**数据生成分布**(data generating distribution)，记作$p_{data}$。
　　我们能观察到的训练误差和泛化误差之间的直接关系就是，训练误差的期望等于测试误差的期望。假设我们从一个$p(x, y)$中重复采样生成训练集和测试集，对于一个固定的模型参数$w$，训练误差的期望等于泛化误差的期望，因为此时测试集和训练集只是名字不同而已，其实其还都是满足数据生成分布的。
　　但是在实际的应用过程中，不会去固定参数，然后去采样两个数据集。**我们先采样训练集，然后减小训练误差得到参数后，再在测试集中验证**，这个过程中，就会发生**测试误差的期望大于训练误差的期望的情况**。以下是决定机器学习算法效果是否好的因素：
1. 降低训练误差。
2. 缩小训练误差与测试误差之间的差距。

这俩个因素分别对应了机器学习的两个大挑战：**欠拟合**(underfitting)和**过拟合**(overfitting)。欠拟合指的是模型的训练误差过大，过拟合指的是训练误差与测试误差的差距过大。

# 容量(capacity)
　　通过调节机器学习模型的**容量**，可以控制模型是否偏于**过拟合**还是**欠拟合**，**容量**从本质上说是描述了整个模型的拟合能力的大小。如果容量不足，模型将不能够很好地表示数据，表现为欠拟合；如果容量太大，那么模型就很容易过分拟合数据，**因为其记住了不适合与测试集的训练集特性**，表现为过拟合。因此控制好模型的容量是一个关键问题。
　　容量的控制可以通过多种方法控制，包括：
* **控制模型的假设空间**。
* **添加正则项对模型进行偏好排除**。

　　我们有个结论：
> 当机器学习算法的容量适合于所执行任务的复杂度和所提供训练数据的数量时，
  算法效果通常会最佳。 容量不足的模型不能解决复杂任务。 容量高的模型能够解决
  复杂的任务，但是当其容量高于任务所需时，有可能会过拟合。
  
　　统计学习方法理论提供了量化模型的容量的不同方法，其中最为出名的是**Vapnik-Chervonenkis 维度**(Vapnik-Chervonenkis dimension)。
　　量化模型的容量使得统计学习理论可以进行量化预测。统计学习理论中最重要的结论阐述了**训练误差和泛化误差之间差异的上界随着模型容量增长而增长，但随着训练样本增多而下降**。我们必须记住：

> 虽然更简单的函数更可能泛化（训练误差和测试误差的差距小），但我们仍然需要选择一个充分复杂的假设以达到低的训练误差。通常，当模型容量上升时，训练误差会下降，直到其渐近最小可能误差（假设误差度量有最小值）。通常， 泛化误差是一个关于模型容量的 U 形曲线函数。

![error_capacity][error_capacity]


## 控制模型的假设空间
　　假设空间(hypothesis space)指的是算法可以作为解决方案的函数集合，比如在线性回归中，其假设空间为其输入的所有线性组合$f(x)=\theta^TX$，而广义线性回归则包括了多项式函数而非仅有线性函数，如$f(x)=\sum_{i=0}^N w_ix^i$。像广义线性回归模型对线性回归模型的补充，则就扩大了模型的容量，增加了其表达能力，也使得其更容易过拟合。
![fitting_status][fitting_status]
*Figure 1， 线性回归模型，全部是线性组合。Figure 2， 添加了二次项的广义线性回归模型。Figure 3， 高达9次的广义线性回归模型。*

　　事实上，模型的最大容量被称为**表示容量**(representational capacity)，指的是通过调节参数降低训练目标时，学习算法可以从哪些函数族中选择函数。实际上，从这些函数中挑选出最优函数是一个极为困难的事情，额外的限制，比如优化算法的不完美，使得模型的**有效容量**(effective capacity)可能会小于表示容量。

**我们将在下一节专门讨论关于正则化的应用，与其在降低泛化误差的原理。**




[fitting_status]: ./imgs/fitting_status.png
[error_capacity]: ./imgs/error_capacity.png









