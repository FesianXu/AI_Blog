<div align='center'>
    讨论数据增强（data augmentation）的有效性
</div>

<div align='right'>
    FesianXu 20210216 at Baidu intern
</div>

# 前言

在知乎上遇到了一个问题并且进行了简单的回答，[如何证明数据增强（Data Augmentation）有效性？](https://www.zhihu.com/question/444425866/answer/1732551386) 这个问题其实蛮有意思的，以此为展开简要记录下在图像领域常用的一些数据增强方法。本文参考了论文[1]。 **如有谬误请联系指出，本文遵守[ CC 4.0 BY-SA ](http://creativecommons.org/licenses/by-sa/4.0/)版权协议，转载请联系作者并注明出处，谢谢**。

$\nabla$ 联系方式：

**e-mail**: FesianXu@gmail.com

**github**: https://github.com/FesianXu

**知乎专栏**: [计算机视觉/计算机图形理论与应用](https://zhuanlan.zhihu.com/c_1265262560611299328)

**微信公众号**：

![qrcode][qrcode]

----



# 学习模型并非只是模型

在知乎问题[2]中，提问者提出问题：

> 数据增强后模型识别准确率的提高可不可能只是因为同一图片增强前后具有相似性而导致的？模型的泛化能力是否有了实质上的提高？

笔者在该问题下给出了回答，提问者会产生这类型疑问的原因在于，他认为学习模型只是一个模型，而数据，损失函数以及学习策略都是独立的，这种概念上的隔离使得读者很容易误认为，数据的归数据，模型的归模型。然而，没有免费午餐定理（No free lunch theorem）告诉我们，没有一种模型是足够强大到可以在所有数据上表现都符合要求的。笔者在之前的博文《 [数据，模型，算法共同决定深度学习模型效果](https://blog.csdn.net/LoseInVain/article/details/105644994) 》[3]中曾经提到，我们应该把一个学习模型分解为三大部分：

1.  数据，也就是$D_{train}$。
2.  模型，其决定了假设空间$\mathcal{H}$。 
3.  算法，如何在指定的假设空间$\mathcal{H}$中去搜索最佳假设以拟合$D_{train}$。 

其中的算法部分又包括了：损失函数和参数更新策略（比如Adam，SGD）等。这三大部分不可以分割开来讨论，在离开了具体数据的情况下，单独讨论模型的泛化性能是没有意义的。在实践中，我们的设计模型的方法论通常是将假设空间设计的足够大，正如《自私的基因》的作者查理德道金斯所说的，“当搜索空间足够大时，有效的搜索就与真正的创造并无二致了”，这句话与博尔赫斯的《巴别图书馆》以及中国古话“文章本天成，妙手偶得之”都具有异曲同工之妙，无非是强调如何在足够大的搜索空间中搜索得到一个足够好的解。 然而，现实世界的数据总是缺乏的，经常不足以支撑起在整个假设空间中搜索得到一个较优解的需求。为了解决这种问题，我们通常需要引入各种各样的先验知识，或从数据着手，或从模型设计着手，或从损失函数，参数更新策略着手，去足够大的假设空间的前提下，减少搜索空间，或者尽量避开那些明显不是解的点。引入先验的方向，如果分摊到数据，模型，算法上，分别有以下三种思路：
1. 数据。在这类型方法中，我们利用先验知识去对$D_{train}$进行数据增广(data augment)，从数据量$I$提高到$\widetilde{I}$，通常$\widetilde{I} >> I$。随后标准的机器学习算法就可以在已经增广过后的数据集上进行。因此，我们可以得到更为精确的假设$h_{\widetilde{I}}$。如Fig 1.1 (a)所示。
2. 模型。这类型方法通过先验知识去约束了假设空间 $\mathcal{H}$ 的复杂度，得到了各位窄小的假设空间$\widetilde{\mathcal{H}}$。如Fig 1.1 (b) 所示。灰色区域已经通过先验知识给排除掉了，因此模型不会考虑往这些方向进行更新，因此，往往需要更少的数据就可以达到更为可靠的经验风险假设。
3. 算法。这类型的方法考虑使用先验知识，指导如何对$\theta$进行搜索。先验知识可以通过提供一个好的参数初始化，或者指导参数的更新步，进而影响参数搜索策略。对于后者来说，其导致的搜索更新步由先验知识和经验风险最小项共同决定。

![dma][dma]

<div align='center'>
    <b>
        Fig 1.1 学习模型由数据，模型和算法三大部分组成，通过对这三部分添加先验，可以减少搜索空间，加快搜索速度和提高搜索结果。
    </b>
</div>

本文着重讨论的是对数据的先验引入，因此讨论数据增广的一些形式：

1. 从训练集中进行数据增强，这个是我们一般认知中的数据增强，比如像素抖动，平移，crop，旋转，镜像等等。
2. 从弱标注或者无标注数据中进行数据增强，这种方法可以用于对模型进行预训练。
3. 从相似的数据集中进行数据增强，这种方法用于Few-shot learning和Transfer learning等。

图像的数据增强操作很多，大致类型如Fig 1.2所示，在本文我们主要介绍最常用的基础图像操作（Basic Image Manipulations）相关的方法。在介绍数据增强前，我们要知道对于整个任务来说，数据增强首先应该是“安全”的，也就是说在数据增强前后图像的标签应该是得以保留的，举个例子，在ImageNet上对图片进行旋转和镜像翻转就是标签安全的，这个不会改变图片类别（比如猫VS狗）；而在MNIST上进行旋转就有可能会改变图片的类别，比如6和9。在进行数据增强前，一定要确保你的操作是label preserve的。

![image_data_augmentation][image_data_augmentation]

<div align='center'>
    <b>
        Fig 1.2 图像相关的数据增强方式。
    </b>
</div>





# Reference

[1].  Shorten, Connor, and Taghi M. Khoshgoftaar. "A survey on image data augmentation for deep learning." *Journal of Big Data* 6.1 (2019): 1-48.

[2]. https://www.zhihu.com/question/444425866/answer/1732551386

[3]. https://blog.csdn.net/LoseInVain/article/details/105644994









[dma]: ./imgs/dma.png
[qrcode]: ./imgs/qrcode.jpg

[image_data_augmentation]: ./imgs/image_data_augmentation.png