<h1 align = "center">随机梯度下降法，批量梯度下降法和小批量梯度下降法</h1>

## 前言
**梯度下降法是深度学习领域用于最优化的常见方法，根据使用的batch大小，可分为随机梯度下降法（SGD）和批量梯度下降法（BGD）和小批量梯度下降法（MBGD），这里简单介绍下并且提供MATLAB代码演示。**
**如有谬误，请联系指正。转载请注明出处。**
*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`
**代码开源：[click]**

# 梯度下降法
　　其基本思想就是使得网络的各个参数朝着优化函数的梯度的反方向更新，因为梯度的方向是增长最速方向，所以梯度的反方向就是减少的最速方向，因此只要设定一个步进$\eta$，然后朝着梯度的反方向下降减小即可将优化函数最小化。公式表达如：
$$
w^l_{ij} := w^l_{ij}-\eta \frac{\partial{C}}{\partial{w^l_{ij}}}
$$


**以下讨论都假设使用的损失函数是MSE损失函数，模型为线性回归模型。**
$$
L = \frac{1}{2m} \sum_{i=1}^m (y-h(x^{(i)}))^2，m是样本总数
$$
$$
h(x_i) = \theta^Tx^{(i)}+b，其中x^{(i)}为第i个样本，x^{(i)} \in R^n
$$

## 批量梯度下降法(BGD)
　　批量梯度下降法(Batch Gradient Descent, BGD)是梯度下降法的最原始的形式，其特点就是**每一次训练迭代都需要利用所有的训练样本**，其表达式为：
$$
\frac{\partial{L}}{\partial{\theta_j}} = 
-\frac{1}{m}\sum_{i=1}^m   [(y-h(x^{(i)}))x^{(i)}_j]，x^{(i)}_j是第i个样本的第j个输入分量
$$ 
$$
\theta_j := \theta_j+\eta \frac{1}{m}\sum_{i=1}^m   [(y-h(x^{(i)}))x^{(i)}_j]
$$
　　**可以发现，对于每一个参数的更新都需要利用到所有的m个样本，这样会导致训练速度随着训练样本的增大产生极大地减慢，不适合于大规模训练样本的场景，但是，其解是全局最优解，精度高。**


## 随机梯度下降法(SGD)
　　随机梯度下降法(Stochastic Gradient Descent, SGD)是梯度下降法的改进形式之一，其特点就是**每一次训练迭代只需要训练样本集中的一个样本**，其表达式为：
$$
\frac{\partial{L}}{\partial{\theta_j}} = 
-(y-h(x))x_j， x_j是选定样本的第j个输入分量
$$
$$
\theta_j := \theta_j+\eta (y-h(x))x_j
$$
　　**和BGD进行对比可以发现，其参数更新过程中，只需要选取训练集中的一个训练样本，因此训练速度快，但是因为其只是利用了训练集的一部分知识，因此其解为局部最优解，精度较低。**


## 小批量梯度下降法(MBGD)
　　小批量梯度下降法(Mini-Batch Gradient Descent, MBGD)是结合了SGD和BGD的一种改进版本，既有训练速度快，也有精度较高的特点，其基本特点就是**每一次训练迭代在训练集中随机采样batch_size个样本**，其表达式为：
$$
\frac{\partial{L}}{\partial{\theta_j}} = 
-\frac{1}{M}\sum_{i=1}^M   [(y-h(x^{(i)}))x^{(i)}_j]，其中M为batch\_size
$$
$$
\theta_j := \theta_j+\eta \frac{1}{M}\sum_{i=1}^M   [(y-h(x^{(i)}))x^{(i)}_j]
$$
　　**这个改进版本在深度学习的网络训练中有着广泛地应用，因为其既有较高的精度，也有较快的训练速度。**






