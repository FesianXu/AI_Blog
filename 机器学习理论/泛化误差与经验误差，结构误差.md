<div align=center>
<font size="6"><b>经验误差，泛化误差
</b></font> 
</div>

# 前言
**我们在上篇博文 [《机器学习模型的容量，过拟合与欠拟合》 ][ref_1]中曾经提到过模型的泛化问题，指的就是描述一个模型在未见过的数据中的表现能力。这里再提出了，用于比较经验误差。**
*联系方式：*
**e-mail**: `FesianXu@163.com`
**QQ**: `973926198`
**github**: `https://github.com/FesianXu`

****************************************************************************************************


假设我们现在有数据集$D=\{(x_1,y_1), (x_2,y_2),\cdots,(x_i,y_i)\}, i=N$,其中$N$是数据集的大小，$x_i$为数据的属性[^1]，$y_i$为标签。假设有$y_i \in \cal Y$，$x_i \in \cal X, \rm i =1,2,\cdots,N$，假设$\cal X$中的所有样本都满足一个隐含的，未知的分布$\cal D$，也就是说$D$中的所有样本都是从$\cal D$中**独立同分布(i.i.d)**地采样的。
然后假设$h$是算法$\cal L$学习到的从$\cal X$到$\cal Y$的映射，$y=h(x)$，并且有$h \in \cal H$，其中$\cal H$为算法$\cal L$的假设空间。我们可以定义映射$h$的**泛化误差(generalization error)**:
$$
E(h; \cal D) = \rm P_{x \sim \cal D} \rm(h(x) \neq y)
\tag{1.1}
$$
因为我们无法观察到整个分布$\cal D$，只能观察到独立同分布采样后的$D$，因此我们需要定义**经验误差(empirical error)**:
$$
\hat E(h;\cal D) = \rm \frac{1}{N} \sum_{i=1}^N 1(h(x_i) \neq y_i),x_i \in D
\tag{1.2}
$$
其中的$1(\cdot)$表示当条件符合时输出1，否则输出0。由于$D$是$\cal D$的独立同分布采样，因此$h$的经验误差的期望等于泛化误差。



****************************************************************************************************
# 引用：
1. [《机器学习模型的容量，过拟合与欠拟合》 CSDN][ref_1]
2. [《机器学习（四）经验风险与结构风险》 CSDN][ref_2]
3. [《机器学习》 周志华著][ref_3]



<!--引用-->
[ref_1]: http://blog.csdn.net/loseinvain/article/details/78108990
[ref_2]: http://blog.csdn.net/u013709270/article/details/53997686
[ref_3]: https://book.douban.com/subject/26708119/

<!--脚注-->
[^1]: 数据的属性指的是数据的最原始的特征，比如图片的原始像素点，而数据的特征大多指的是属性经过特定的操作的数据，如图片的像素点经过CNN卷积之后得到的特征。广义来说，数据的属性和特征没有区别。


